{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34179b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quasirandom import SobolEngine\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "from functools import cache\n",
    "\n",
    "class PDEProblem(ABC):\n",
    "    def __init__(\n",
    "            self,\n",
    "            name: str,\n",
    "            input_vars: List[str] = ['x'],\n",
    "            output_vars: List[str] = ['u'],\n",
    "            time_var: Optional[str] = None,\n",
    "            kappa_name: str = \"kappa\",\n",
    "            default_kappa_value: float = 1.0\n",
    "        ):\n",
    "        self.name: str = name\n",
    "        self.input_vars: List[str] = sorted(list(set(input_vars)))\n",
    "        self.output_vars: List[str] = output_vars\n",
    "        self.time_var: Optional[str] = time_var\n",
    "\n",
    "        self.output_dim: int = len(self.output_vars)\n",
    "        self.spatial_domain_dim: int = len(self.input_vars) - (1 if time_var else 0)\n",
    "        self.time_dependent: bool = bool(time_var)\n",
    "\n",
    "        self.kappa_name: str = kappa_name\n",
    "        self.default_kappa_value: float = default_kappa_value\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping input variable names to their (min, max) bounds.\n",
    "        Example: {'x': (0.0, 1.0), 't': (0.0, 2.0)}\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def pde_residual(\n",
    "            self,\n",
    "            inputs: Dict[str, torch.Tensor],\n",
    "            model_outputs: torch.Tensor, # Shape: (batch, output_dim)\n",
    "            derivatives: Dict[str, torch.Tensor], # Keys like 'd(u)_dx(1)', 'd2(u)_dx(2)', 'd(v)_dt(1)' etc.\n",
    "            kappa_value: float\n",
    "        ) -> torch.Tensor: # Expected shape: (batch, num_pde_equations)\n",
    "        \"\"\"\n",
    "        Calculates the PDE residual(s).\n",
    "        - model_outputs: Tensor of shape (batch_size, self.output_dim)\n",
    "        - derivatives: Dictionary where keys might be 'd(out_var)_d(in_var)(order)'\n",
    "                       e.g., 'd1u_dx1' for du/dx, 'd2v_dydt1' for d^2v/dydt.\n",
    "        Should return a tensor where each column is the residual of one PDE equation.\n",
    "        For scalar PDEs, this will be (batch_size, 1).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def boundary_conditions(\n",
    "        self,\n",
    "        inputs_bc: Dict[str, torch.Tensor],\n",
    "        model_outputs_bc: torch.Tensor, # Shape: (batch_bc, output_dim),\n",
    "        derivatives_bc: Dict[str, torch.Tensor], # Keys like 'd(u)_dx(1)', 'd(v)_dt(1)' etc.\n",
    "        model: nn.Module,\n",
    "        kappa_value: float\n",
    "        ) -> torch.Tensor: # Scalar loss term\n",
    "        pass\n",
    "\n",
    "    def initial_conditions(\n",
    "            self,\n",
    "            inputs_ic: Dict[str, torch.Tensor],\n",
    "            model_outputs_ic: torch.Tensor, # Shape: (batch_ic, output_dim)\n",
    "            derivatives_ic: Dict[str, torch.Tensor], # Keys like 'd(u)_dx(1)', 'd(v)_dt(1)' etc.\n",
    "            model: nn.Module,\n",
    "            kappa_value: float\n",
    "        ) -> torch.Tensor: # Scalar loss term\n",
    "        if not self.time_dependent:\n",
    "            device = 'cpu'\n",
    "            if model:\n",
    "                try: device = next(model.parameters()).device\n",
    "                except StopIteration: pass\n",
    "            elif isinstance(model_outputs_ic, torch.Tensor):\n",
    "                device = model_outputs_ic.device\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        raise NotImplementedError(\"Initial conditions must be implemented for time-dependent PDEs.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_ground_truth(self,\n",
    "                         inputs: Dict[str, torch.Tensor],\n",
    "                         kappa_value: float) -> Optional[torch.Tensor]: # Shape: (batch, output_dim)\n",
    "        pass\n",
    "\n",
    "    def get_collocation_points(self,\n",
    "                               num_points: int,\n",
    "                               kappa_value: float,\n",
    "                               device: Union[str, torch.device] = 'cpu',\n",
    "                               strategy: str = 'uniform') -> Dict[str, torch.Tensor]:\n",
    "        domain_bounds = self.get_domain_bounds()\n",
    "        inputs = {}\n",
    "\n",
    "        if strategy == 'sobol':\n",
    "            num_input_dims_for_sampling = len(self.input_vars)\n",
    "            if num_input_dims_for_sampling == 0: # Should not happen for collocation\n",
    "                 return {}\n",
    "            sobol = SobolEngine(dimension=num_input_dims_for_sampling, scramble=True)\n",
    "            # Move Sobol samples to target device after generation\n",
    "            samples_0_1 = sobol.draw(num_points).to(device)\n",
    "\n",
    "        for i, var_name in enumerate(self.input_vars):\n",
    "            if var_name not in domain_bounds:\n",
    "                raise ValueError(f\"Domain bounds not defined for variable: {var_name}\")\n",
    "            var_min, var_max = domain_bounds[var_name]\n",
    "\n",
    "            if strategy == 'uniform':\n",
    "                samples_var = torch.rand(num_points, 1, device=device) * (var_max - var_min) + var_min\n",
    "            elif strategy == 'sobol':\n",
    "                samples_var = samples_0_1[:, i:i+1] * (var_max - var_min) + var_min\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Collocation sampling strategy '{strategy}' not implemented for variable '{var_name}'.\")\n",
    "\n",
    "            inputs[var_name] = samples_var.requires_grad_(True)\n",
    "        return inputs\n",
    "\n",
    "    def get_boundary_points_hyperrect(self,\n",
    "                            num_points_per_face: int,\n",
    "                            kappa_value: float,\n",
    "                            device: Union[str, torch.device] = 'cpu',\n",
    "                            strategy: str = 'uniform') -> Dict[str, torch.Tensor]:\n",
    "        domain_bounds = self.get_domain_bounds()\n",
    "        all_bc_inputs = {v: [] for v in self.input_vars}\n",
    "        # Spatial vars are input_vars excluding the time_var\n",
    "        spatial_vars = [v for v in self.input_vars if v != self.time_var]\n",
    "\n",
    "        if not spatial_vars: # No spatial dimensions, so no spatial boundaries\n",
    "            return {v: torch.empty(0,1,device=device).detach() for v in self.input_vars}\n",
    "\n",
    "        samples_bc_other_dims = None\n",
    "        num_dims_to_sample_on_face = len(spatial_vars) - 1 + (1 if self.time_dependent else 0)\n",
    "\n",
    "        if strategy == 'sobol' and num_dims_to_sample_on_face > 0:\n",
    "            sobol_bc = SobolEngine(dimension=num_dims_to_sample_on_face, scramble=True)\n",
    "            samples_bc_other_dims = sobol_bc.draw(num_points_per_face).to(device)\n",
    "        elif strategy != 'uniform' and strategy != 'sobol': # if strategy is not uniform and sobol setup failed or not chosen\n",
    "            raise NotImplementedError(f\"Boundary sampling strategy '{strategy}' not supported.\")\n",
    "\n",
    "\n",
    "        for fixed_var_name in spatial_vars:\n",
    "            other_sampling_vars = [v for v in spatial_vars if v != fixed_var_name]\n",
    "            if self.time_dependent:\n",
    "                other_sampling_vars.append(self.time_var)\n",
    "\n",
    "            for boundary_value in domain_bounds[fixed_var_name]: # For min and max of this fixed_var\n",
    "                current_face_inputs = {}\n",
    "                current_face_inputs[fixed_var_name] = torch.full((num_points_per_face, 1),\n",
    "                                                                boundary_value, dtype=torch.float32, device=device)\n",
    "\n",
    "                sample_idx = 0\n",
    "                for other_var_name in other_sampling_vars:\n",
    "                    ov_min, ov_max = domain_bounds[other_var_name]\n",
    "                    if strategy == 'sobol' and samples_bc_other_dims is not None:\n",
    "                        current_face_inputs[other_var_name] = samples_bc_other_dims[:, sample_idx:sample_idx+1] * (ov_max - ov_min) + ov_min\n",
    "                        sample_idx +=1\n",
    "                    else: # Default to uniform if Sobol not applicable or not chosen\n",
    "                        current_face_inputs[other_var_name] = torch.rand(num_points_per_face, 1, device=device) * (ov_max - ov_min) + ov_min\n",
    "\n",
    "                # Append points for this face to the main list\n",
    "                for var_n in self.input_vars:\n",
    "                    all_bc_inputs[var_n].append(current_face_inputs[var_n])\n",
    "\n",
    "        # Concatenate points from all faces\n",
    "        final_bc_inputs = {}\n",
    "        for var_n in self.input_vars:\n",
    "            if all_bc_inputs[var_n]: # If any points were added for this variable\n",
    "                final_bc_inputs[var_n] = torch.cat(all_bc_inputs[var_n], dim=0).detach() # BC points usually don't need grad\n",
    "            else: # Should only happen if input_vars is empty or logic error\n",
    "                final_bc_inputs[var_n] = torch.empty(0,1,device=device).detach()\n",
    "        return final_bc_inputs\n",
    "\n",
    "    def get_boundary_points_general(self,\n",
    "                                       num_total_points: int, # Note: parameter name change\n",
    "                                       kappa_value: float,\n",
    "                                       device: Union[str, torch.device] = 'cpu',\n",
    "                                       strategy: str = 'uniform' # Strategy for sampling on the general boundary\n",
    "                                      ) -> Optional[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        To be implemented by subclasses for non-rectangular/complex domains.\n",
    "        Should return points lying *on* the boundary.\n",
    "        Returns None to indicate this method is not implemented or not applicable,\n",
    "        allowing fallback to get_boundary_points_hyperrect.\n",
    "        \"\"\"\n",
    "        return None  # Indicating no general boundary points available\n",
    "\n",
    "    def get_initial_points(self,\n",
    "                           num_points: int,\n",
    "                           kappa_value: float,\n",
    "                           device: Union[str, torch.device] = 'cpu',\n",
    "                           strategy: str = 'uniform') -> Dict[str, torch.Tensor]:\n",
    "        if not self.time_dependent:\n",
    "            return {v: torch.empty(0,1,device=device).requires_grad_(False) for v in self.input_vars}\n",
    "\n",
    "        domain_bounds = self.get_domain_bounds()\n",
    "        inputs = {}\n",
    "\n",
    "        t_initial_val = domain_bounds[self.time_var][0]\n",
    "        inputs[self.time_var] = torch.full((num_points, 1), t_initial_val, dtype=torch.float32, device=device)\n",
    "\n",
    "        spatial_vars = [v for v in self.input_vars if v != self.time_var]\n",
    "        if strategy == 'sobol' and spatial_vars: # only use sobol if there are spatial vars to sample\n",
    "            sobol_ic = SobolEngine(dimension=len(spatial_vars), scramble=True)\n",
    "            samples_0_1_ic = sobol_ic.draw(num_points).to(device)\n",
    "        elif strategy != 'uniform' and strategy != 'sobol':\n",
    "             raise NotImplementedError(f\"IC sampling strategy '{strategy}' not supported.\")\n",
    "\n",
    "\n",
    "        for i, var_name in enumerate(spatial_vars):\n",
    "            var_min, var_max = domain_bounds[var_name]\n",
    "            if strategy == 'uniform':\n",
    "                inputs[var_name] = torch.rand(num_points, 1, device=device) * (var_max - var_min) + var_min\n",
    "            elif strategy == 'sobol' and spatial_vars: # check spatial_vars again for safety\n",
    "                inputs[var_name] = samples_0_1_ic[:, i:i+1] * (var_max - var_min) + var_min\n",
    "            # No else needed if strategy check is done above\n",
    "\n",
    "        # Ensure all input_vars keys are present, even if fixed (like time)\n",
    "        for var_name in self.input_vars:\n",
    "            if var_name not in inputs: # e.g. if only time_var and no spatial_vars\n",
    "                 if var_name == self.time_var: continue # already handled\n",
    "                 # This case should be rare if input_vars is setup correctly with domain_bounds\n",
    "                 inputs[var_name] = torch.empty(num_points, 1, device=device) # or handle error\n",
    "\n",
    "            inputs[var_name].requires_grad_(False) # IC coords generally don't need grad\n",
    "        return inputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary specifying derivative requirements for each output variable.\n",
    "        Structure:\n",
    "          {\n",
    "            'output_var_name_1': { # For the first output variable (e.g., 'u')\n",
    "                # Simple derivatives:\n",
    "                ('input_var_for_deriv',): order,  # e.g., ('x',): 2 for d2(u)/dx2\n",
    "                # Mixed derivatives (sequence of differentiation):\n",
    "                ('input_var_1', 'input_var_2', ...): 1, # e.g., ('x', 'y'): 1 for d/dy(d(u)/dx)\n",
    "                                                       # The value (e.g., 1) indicates one application\n",
    "                                                       # of this sequence of differentiations.\n",
    "            },\n",
    "            'output_var_name_2': { ... } # For the second output variable (e.g., 'v')\n",
    "          }\n",
    "        Example for -u_xx - u_yy = f (output_vars=['u']):\n",
    "          {'u': {('x',): 2, ('y',): 2}}\n",
    "        Example for u_t + v_x = 0, v_t + u_x = 0 (output_vars=['u', 'v']):\n",
    "          {\n",
    "            'u': {('t',): 1},\n",
    "            'v': {('x',): 1, ('t',): 1} # Here u_x is not directly a derivative of 'v',\n",
    "                                        # but if 'v' appears in an equation with u_x,\n",
    "                                        # the PDE residual itself handles fetching u_x.\n",
    "                                        # This dict is about derivatives OF the key output_var_name.\n",
    "                                        # Let's refine this point below.\n",
    "          }\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_required_derivative_orders_for_bc(self) -> Optional[Dict[str, Dict[Tuple[str, ...], int]]]:\n",
    "        \"\"\"Specifies derivatives needed for boundary condition evaluation. Returns None if none needed.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_required_derivative_orders_for_ic(self) -> Optional[Dict[str, Dict[Tuple[str, ...], int]]]:\n",
    "        \"\"\"Specifies derivatives needed for initial condition evaluation. Returns None if none needed.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def calculate_specific_observables(self,\n",
    "                                       inputs: Dict[str, torch.Tensor],\n",
    "                                       model_outputs: torch.Tensor,\n",
    "                                       ground_truth_outputs: Optional[torch.Tensor],\n",
    "                                       kappa_value: float) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculates PDE-specific physical observables and their errors.\n",
    "        To be implemented by subclasses if relevant.\n",
    "        Args:\n",
    "            inputs: Dictionary of input tensors for the test grid.\n",
    "            model_outputs: Tensor of model predictions on the test grid.\n",
    "            ground_truth_outputs: Tensor of ground truth solutions on the test grid (if available).\n",
    "            kappa_value: Current hardness parameter.\n",
    "        Returns:\n",
    "            A dictionary of observable names to their scalar values (e.g., errors).\n",
    "            Example: {'soliton_amplitude_error': 0.01, 'shock_speed_error': 0.05}\n",
    "        \"\"\"\n",
    "        return {} # Default implementation returns no specific observables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dbb8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "\n",
    "def create_pinn_model(\n",
    "    input_dim: int,\n",
    "    output_dim: int,\n",
    "    n_neurons_per_layer: int,\n",
    "    n_hidden_layers: int = 1, # Default to SLN\n",
    "    activation_str: str = \"tanh\",\n",
    "    device: Union[str, torch.device] = 'cpu'\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Creates a feedforward neural network (PINN model).\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input (e.g., 1 for u(x), 2 for u(x,t)).\n",
    "        output_dim (int): Dimension of the output (e.g., 1 for scalar u).\n",
    "        n_neurons_per_layer (int): Number of neurons in each hidden layer.\n",
    "        n_hidden_layers (int): Number of hidden layers. Default is 1.\n",
    "        activation_str (str): Activation function to use ('tanh', 'relu', 'sigmoid', 'leakyrelu').\n",
    "                              Default is 'tanh'.\n",
    "        device (Union[str, torch.device]): Device to send the model to ('cpu' or 'cuda').\n",
    "                                           Default is 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The PyTorch neural network model (nn.Sequential).\n",
    "    \"\"\"\n",
    "    layers: list[nn.Module] = []\n",
    "\n",
    "    if n_hidden_layers == 0: # Special case: linear model (no hidden layers)\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "    else:\n",
    "        layers.append(nn.Linear(input_dim, n_neurons_per_layer))\n",
    "\n",
    "        # Activation function selection\n",
    "        if activation_str.lower() == 'tanh':\n",
    "            activation_fn: nn.Module = nn.Tanh()\n",
    "        elif activation_str.lower() == 'relu':\n",
    "            activation_fn: nn.Module = nn.ReLU()\n",
    "        elif activation_str.lower() == 'sigmoid':\n",
    "            activation_fn: nn.Module = nn.Sigmoid()\n",
    "        elif activation_str.lower() == 'leakyrelu':\n",
    "            activation_fn: nn.Module = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(n_neurons_per_layer, n_neurons_per_layer))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        # Output layer (connects last hidden layer to output_dim)\n",
    "        layers.append(nn.Linear(n_neurons_per_layer, output_dim))\n",
    "\n",
    "    model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "    # Apply initializations\n",
    "    for i, layer in enumerate(model):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            if activation_str.lower() == 'tanh' or activation_str.lower() == 'sigmoid':\n",
    "                nn.init.xavier_normal_(layer.weight) # Glorot normal\n",
    "            elif activation_str.lower() == 'relu' or activation_str.lower() == 'leakyrelu':\n",
    "                # For Kaiming, if the next layer is an activation, use that info.\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu' if activation_str.lower() == 'relu' else 'leaky_relu')\n",
    "\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a5f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, pde_problem: 'PDEProblem', optimizer_str=\"adam\", learning_rate=1e-3, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.pde_problem = pde_problem # Type hint for clarity\n",
    "        self.device = device\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        if optimizer_str.lower() == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer_str.lower() == \"lbfgs\":\n",
    "            self.optimizer = optim.LBFGS(self.model.parameters(), lr=self.lr, max_iter=20, line_search_fn=\"strong_wolfe\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_str}\")\n",
    "\n",
    "        self.optimizer_str = optimizer_str\n",
    "        self.epoch_wise_log = []\n",
    "\n",
    "    def _prepare_model_input(self, inputs_dict: dict) -> torch.Tensor | None:\n",
    "        \"\"\"\n",
    "        Prepares a single tensor input for the model from the inputs_dict.\n",
    "        The order of concatenation is defined by self.pde_problem.input_vars.\n",
    "        \"\"\"\n",
    "        if not inputs_dict:\n",
    "            return None # Or handle as appropriate if model expects input even for empty dict\n",
    "\n",
    "        ordered_input_tensors = []\n",
    "        for var_name in self.pde_problem.input_vars:\n",
    "            if var_name in inputs_dict:\n",
    "                ordered_input_tensors.append(inputs_dict[var_name])\n",
    "            else:\n",
    "                # This should ideally not happen if PDEProblem methods are consistent\n",
    "                raise ValueError(f\"Input variable '{var_name}' expected by PDEProblem.input_vars \"\n",
    "                                 f\"but not found in provided inputs_dict keys: {list(inputs_dict.keys())}\")\n",
    "\n",
    "        if not ordered_input_tensors: # Should be caught by the first check if inputs_dict is empty\n",
    "             return torch.empty(0, device=self.device)\n",
    "\n",
    "        return torch.cat(ordered_input_tensors, dim=1)\n",
    "\n",
    "    def _compute_derivatives(self,\n",
    "                             inputs_dict_with_grad: Dict[str, torch.Tensor],\n",
    "                             model_outputs_tensor: torch.Tensor,\n",
    "                             required_specs: Dict[str, Dict[Tuple[str, ...], int]] = None\n",
    "                             ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes derivatives based on pde_problem.get_required_derivative_orders().\n",
    "        model_outputs_tensor has shape (batch, pde_problem.output_dim)\n",
    "        Derivatives are taken with respect to the individual tensors in inputs_dict_with_grad.\n",
    "\n",
    "        Returns a dictionary of derivatives.\n",
    "        Naming convention examples:\n",
    "        - d(u)_dx(1)       for first derivative of 'u' wrt 'x'\n",
    "        - d2(u)_dx(2)      for second derivative of 'u' wrt 'x'\n",
    "        - d(v)_dt(1)       for first derivative of 'v' wrt 't'\n",
    "        - d(u)_dx(1)dy(1)  for d/dy(du/dx)\n",
    "        \"\"\"\n",
    "        derivatives: Dict[str, torch.Tensor] = {}\n",
    "        if required_specs is None:\n",
    "            required_specs = self.pde_problem.get_required_derivative_orders()\n",
    "        if not required_specs:\n",
    "            return derivatives\n",
    "        output_var_names = self.pde_problem.output_vars\n",
    "\n",
    "        for out_idx, out_var_name in enumerate(output_var_names):\n",
    "            if out_var_name not in required_specs: # If no derivatives are listed for this output var\n",
    "                continue\n",
    "\n",
    "            # current_output_component is (batch_size, 1)\n",
    "            current_output_component = model_outputs_tensor[:, out_idx:out_idx+1]\n",
    "\n",
    "            spec_for_this_output_var = required_specs[out_var_name]\n",
    "\n",
    "            for input_var_sequence, order_val in spec_for_this_output_var.items():\n",
    "                # input_var_sequence is a tuple, e.g., ('x',) or ('x', 't')\n",
    "                # order_val for simple derivatives is the max order, e.g., 2 for d2u/dx2\n",
    "                # order_val for mixed sequence is typically 1 (one application of the sequence)\n",
    "\n",
    "                if not isinstance(input_var_sequence, tuple) or not input_var_sequence:\n",
    "                    raise ValueError(f\"Invalid input_var_sequence: {input_var_sequence} for {out_var_name}\")\n",
    "\n",
    "                # --- Handle Simple Derivatives (e.g., ('x',): 2 for d2u/dx2) ---\n",
    "                if len(input_var_sequence) == 1:\n",
    "                    input_var_name_for_deriv = input_var_sequence[0]\n",
    "                    max_order = order_val\n",
    "\n",
    "                    if input_var_name_for_deriv not in inputs_dict_with_grad:\n",
    "                        raise RuntimeError(f\"Input variable '{input_var_name_for_deriv}' needed for derivative of '{out_var_name}' \"\n",
    "                                           f\"not found in inputs_dict_with_grad: {list(inputs_dict_with_grad.keys())}\")\n",
    "                    input_tensor_for_grad = inputs_dict_with_grad[input_var_name_for_deriv]\n",
    "                    if not input_tensor_for_grad.requires_grad:\n",
    "                        raise RuntimeError(f\"Input tensor for '{input_var_name_for_deriv}' does not require grad.\")\n",
    "\n",
    "                    temp_deriv_target = current_output_component\n",
    "                    for o in range(1, max_order + 1):\n",
    "                        grads = torch.autograd.grad(\n",
    "                            outputs=temp_deriv_target,\n",
    "                            inputs=input_tensor_for_grad,\n",
    "                            grad_outputs=torch.ones_like(temp_deriv_target),\n",
    "                            create_graph=True,\n",
    "                            retain_graph=True,\n",
    "                            allow_unused=False # Be strict initially\n",
    "                        )[0]\n",
    "                        if grads is None:\n",
    "                            raise RuntimeError(f\"Gradient for d{o}({out_var_name})_d({input_var_name_for_deriv}){o} was None.\")\n",
    "\n",
    "                        deriv_name = f\"d{o}({out_var_name})_d{input_var_name_for_deriv}({o})\"\n",
    "                        derivatives[deriv_name] = grads\n",
    "                        temp_deriv_target = grads\n",
    "\n",
    "                # --- Handle Mixed Derivatives (e.g., ('x', 't'): 1 for d/dt(du/dx)) ---\n",
    "                elif len(input_var_sequence) > 1:\n",
    "                    if order_val != 1:\n",
    "                        # For now, assume mixed derivative specs like ('x','y'):1 mean one application of d/dy(d/dx(...))\n",
    "                        # Higher order_val for mixed could mean repeated application of the sequence, but that's rare.\n",
    "                        print(f\"Warning: Mixed derivative for {out_var_name} wrt {input_var_sequence} has order_val {order_val} != 1. Interpreting as 1 application.\")\n",
    "\n",
    "                    temp_deriv_target = current_output_component\n",
    "\n",
    "                    # Build the name like \"d(u)_dx(1)dy(1)\"\n",
    "                    # The number before (out_var_name) will be len(input_var_sequence)\n",
    "                    name_prefix = f\"d{len(input_var_sequence)}({out_var_name})_d\"\n",
    "                    name_suffix_parts = []\n",
    "\n",
    "                    for i, invar_name in enumerate(input_var_sequence):\n",
    "                        if invar_name not in inputs_dict_with_grad:\n",
    "                            raise RuntimeError(f\"Input variable '{invar_name}' for mixed derivative of '{out_var_name}' \"\n",
    "                                               f\"not in inputs_dict_with_grad.\")\n",
    "                        input_tensor_for_grad = inputs_dict_with_grad[invar_name]\n",
    "                        if not input_tensor_for_grad.requires_grad:\n",
    "                             raise RuntimeError(f\"Input tensor for mixed deriv '{invar_name}' does not require grad.\")\n",
    "\n",
    "                        grads = torch.autograd.grad(\n",
    "                            outputs=temp_deriv_target,\n",
    "                            inputs=input_tensor_for_grad,\n",
    "                            grad_outputs=torch.ones_like(temp_deriv_target),\n",
    "                            create_graph=True, # Must be true if any further grads in sequence\n",
    "                            retain_graph=True, # Must be true\n",
    "                            allow_unused=False\n",
    "                        )[0]\n",
    "                        if grads is None:\n",
    "                            raise RuntimeError(f\"Mixed derivative part d/d{invar_name} for {out_var_name} failed.\")\n",
    "                        temp_deriv_target = grads\n",
    "                        name_suffix_parts.append(f\"{invar_name}(1)\")\n",
    "\n",
    "                    deriv_name = name_prefix + \"\".join(name_suffix_parts)\n",
    "                    derivatives[deriv_name] = temp_deriv_target\n",
    "        return derivatives\n",
    "\n",
    "    def _calculate_error_metrics_on_test_grid(self, kappa_value, num_test_pts=1001):\n",
    "        self.model.eval()\n",
    "        domain_bounds = self.pde_problem.get_domain_bounds()\n",
    "        test_inputs_dict_for_gt = {} # Populate this as before based on input_vars\n",
    "\n",
    "        # ... (grid generation logic as in your full code for 1D/2D inputs) ...\n",
    "        if len(self.pde_problem.input_vars) == 1:\n",
    "            var_name = self.pde_problem.input_vars[0]\n",
    "            var_min, var_max = domain_bounds[var_name]\n",
    "            test_values_np = np.linspace(var_min, var_max, num_test_pts)\n",
    "            test_values_torch = torch.tensor(test_values_np, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "            test_inputs_dict_for_gt[var_name] = test_values_torch\n",
    "        elif len(self.pde_problem.input_vars) == 2:\n",
    "            var1_name, var2_name = self.pde_problem.input_vars[0], self.pde_problem.input_vars[1]\n",
    "            var1_min, var1_max = domain_bounds[var1_name]\n",
    "            var2_min, var2_max = domain_bounds[var2_name]\n",
    "            pts_per_dim = int(np.sqrt(num_test_pts))\n",
    "            # ... (meshgrid logic) ...\n",
    "            # (ensure num_test_pts is updated based on actual grid size)\n",
    "            var1_vals = torch.linspace(var1_min, var1_max, pts_per_dim, device=self.device)\n",
    "            var2_vals = torch.linspace(var2_min, var2_max, pts_per_dim, device=self.device)\n",
    "            grid_var1, grid_var2 = torch.meshgrid(var1_vals, var2_vals, indexing='ij')\n",
    "            test_inputs_dict_for_gt[var1_name] = grid_var1.reshape(-1, 1)\n",
    "            test_inputs_dict_for_gt[var2_name] = grid_var2.reshape(-1, 1)\n",
    "            num_test_pts = test_inputs_dict_for_gt[var1_name].shape[0]\n",
    "        else:\n",
    "            # For >2D, you'll need to implement a more general grid creation or accept it as an argument\n",
    "            # For now, let's assume we won't hit this for the workshop's core PDEs\n",
    "            print(\"Warning: Test grid generation for >2 input_vars not fully implemented in error metrics.\")\n",
    "            # Fallback or raise error\n",
    "            return {key: float('nan') for key in ['L1_err', 'L2_err', 'Linf_err', 'L1_err_rel',\n",
    "                                                  'L2_err_rel', 'Linf_err_rel', 'PDE_residual_max',\n",
    "                                                  'error_median_abs', 'error_p90_abs']}\n",
    "\n",
    "\n",
    "        test_model_input_tensor = self._prepare_model_input(test_inputs_dict_for_gt)\n",
    "        if test_model_input_tensor is None or test_model_input_tensor.numel() == 0:\n",
    "             print(\"Warning: No test model input tensor generated for error metrics.\")\n",
    "             return {key: float('nan') for key in ['L1_err', 'L2_err', 'Linf_err', 'L1_err_rel',\n",
    "                                                  'L2_err_rel', 'Linf_err_rel', 'PDE_residual_max',\n",
    "                                                  'error_median_abs', 'error_p90_abs']}\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            u_pred_test = self.model(test_model_input_tensor)\n",
    "\n",
    "        u_true_test_torch = self.pde_problem.get_ground_truth(test_inputs_dict_for_gt, kappa_value)\n",
    "\n",
    "        if u_true_test_torch is not None and u_pred_test.shape != u_true_test_torch.shape:\n",
    "            try: u_pred_test = u_pred_test.reshape_as(u_true_test_torch)\n",
    "            except RuntimeError: print(f\"Warning: Cannot reshape u_pred_test for error calc.\")\n",
    "\n",
    "        metrics = {\n",
    "            'L1_err': float('nan'), 'L2_err': float('nan'), 'Linf_err': float('nan'),\n",
    "            'L1_err_rel': float('nan'), 'L2_err_rel': float('nan'), 'Linf_err_rel': float('nan'),\n",
    "            'PDE_residual_max': float('nan'),\n",
    "            'error_median_abs': float('nan'), 'error_p90_abs': float('nan') # New\n",
    "        }\n",
    "\n",
    "        if u_true_test_torch is not None:\n",
    "            error_vec = (u_pred_test - u_true_test_torch).flatten() # Flatten for norms and quantiles\n",
    "            actual_num_test_pts = len(error_vec)\n",
    "            if actual_num_test_pts == 0: actual_num_test_pts = 1\n",
    "\n",
    "            metrics['L1_err'] = torch.linalg.norm(error_vec, ord=1).item() / actual_num_test_pts\n",
    "            metrics['L2_err'] = torch.linalg.norm(error_vec, ord=2).item() / np.sqrt(actual_num_test_pts)\n",
    "            metrics['Linf_err'] = torch.linalg.norm(error_vec, ord=float('inf')).item()\n",
    "\n",
    "            abs_error_vec = torch.abs(error_vec)\n",
    "            metrics['error_median_abs'] = torch.median(abs_error_vec).item()\n",
    "            if actual_num_test_pts > 0 : # Quantile needs at least one element\n",
    "                 metrics['error_p90_abs'] = torch.quantile(abs_error_vec, 0.9).item()\n",
    "\n",
    "            u_true_flat = u_true_test_torch.flatten()\n",
    "            norm_u_true_l1 = torch.linalg.norm(u_true_flat, ord=1)\n",
    "            norm_u_true_l2 = torch.linalg.norm(u_true_flat, ord=2)\n",
    "            norm_u_true_linf = torch.linalg.norm(u_true_flat, ord=float('inf'))\n",
    "\n",
    "            if norm_u_true_l1 > 1e-9: metrics['L1_err_rel'] = torch.linalg.norm(error_vec, ord=1).item() / norm_u_true_l1.item()\n",
    "            if norm_u_true_l2 > 1e-9: metrics['L2_err_rel'] = torch.linalg.norm(error_vec, ord=2).item() / norm_u_true_l2.item()\n",
    "            if norm_u_true_linf > 1e-9: metrics['Linf_err_rel'] = metrics['Linf_err'] / norm_u_true_linf.item()\n",
    "\n",
    "        # Max PDE residual\n",
    "        test_inputs_dict_for_res = {}\n",
    "        for k, v_test in test_inputs_dict_for_gt.items():\n",
    "            if v_test.numel() > 0: # Only process if tensor is not empty\n",
    "                test_inputs_dict_for_res[k] = v_test.clone().detach().requires_grad_(True)\n",
    "\n",
    "        if test_inputs_dict_for_res: # Proceed only if there are inputs for residual calculation\n",
    "            res_model_input_tensor = self._prepare_model_input(test_inputs_dict_for_res)\n",
    "            if res_model_input_tensor is not None and res_model_input_tensor.numel() > 0:\n",
    "                u_pred_for_res = self.model(res_model_input_tensor)\n",
    "                derivatives_for_res = self._compute_derivatives(test_inputs_dict_for_res, u_pred_for_res)\n",
    "                pde_res_vals_on_grid = self.pde_problem.pde_residual(test_inputs_dict_for_res, u_pred_for_res, derivatives_for_res, kappa_value)\n",
    "                if pde_res_vals_on_grid is not None and pde_res_vals_on_grid.numel() > 0 :\n",
    "                    metrics['PDE_residual_max'] = torch.max(torch.abs(pde_res_vals_on_grid.detach())).item()\n",
    "\n",
    "        # Calculate specific observables if the PDEProblem has this method\n",
    "        if hasattr(self.pde_problem, 'calculate_specific_observables'):\n",
    "            try:\n",
    "                specific_obs = self.pde_problem.calculate_specific_observables(\n",
    "                    test_inputs_dict_for_gt, # The dict of input tensors for the test grid\n",
    "                    u_pred_test,             # Model predictions on the test grid\n",
    "                    u_true_test_torch,       # Ground truth on the test grid\n",
    "                    kappa_value\n",
    "                )\n",
    "                if specific_obs and isinstance(specific_obs, dict):\n",
    "                    metrics.update(specific_obs) # Add them to the metrics dict for this epoch\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error calculating specific observables for {self.pde_problem.name}: {e}\")\n",
    "\n",
    "        self.model.train()\n",
    "        return metrics\n",
    "\n",
    "    def _closure_lbfgs(self, collocation_points_dict, bc_points_dict, ic_points_dict, kappa_value, loss_weights):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # PDE Loss\n",
    "        colloc_model_input = self._prepare_model_input(collocation_points_dict)\n",
    "        model_outputs_colloc = self.model(colloc_model_input)\n",
    "        derivatives_colloc = self._compute_derivatives(collocation_points_dict, model_outputs_colloc)\n",
    "        pde_res = self.pde_problem.pde_residual(collocation_points_dict, model_outputs_colloc, derivatives_colloc, kappa_value)\n",
    "        loss_pde = torch.mean(pde_res**2)\n",
    "\n",
    "        # BC Loss\n",
    "        loss_bc = torch.tensor(0.0, device=self.device)\n",
    "        if bc_points_dict: # Check if not empty\n",
    "            bc_model_input = self._prepare_model_input(bc_points_dict)\n",
    "            model_outputs_bc = self.model(bc_model_input)\n",
    "\n",
    "            derivatives_at_bc = {}\n",
    "            bc_deriv_spec = self.pde_problem.get_required_derivative_orders_for_bc()\n",
    "            if bc_deriv_spec:\n",
    "                # Ensure bc_points_dict tensors used for derivatives have requires_grad=True\n",
    "                grad_enabled_bc_points_dict = {\n",
    "                    k: v.clone().detach().requires_grad_(True) for k,v in bc_points_dict.items()\n",
    "                }\n",
    "                grad_enabled_bc_model_input = self._prepare_model_input(grad_enabled_bc_points_dict)\n",
    "                model_outputs_bc_for_deriv = self.model(grad_enabled_bc_model_input) # Re-evaluate for graph\n",
    "\n",
    "                derivatives_at_bc = self._compute_derivatives(grad_enabled_bc_points_dict, model_outputs_bc_for_deriv)\n",
    "\n",
    "            loss_bc = self.pde_problem.boundary_conditions(bc_points_dict, model_outputs_bc, derivatives_at_bc, self.model, kappa_value)\n",
    "\n",
    "        # IC Loss\n",
    "        loss_ic = torch.tensor(0.0, device=self.device)\n",
    "        if self.pde_problem.time_dependent and ic_points_dict:\n",
    "            ic_model_input = self._prepare_model_input(ic_points_dict)\n",
    "            model_outputs_ic = self.model(ic_model_input)\n",
    "\n",
    "            derivatives_at_ic = {}\n",
    "            ic_deriv_spec = self.pde_problem.get_required_derivative_orders_for_ic()\n",
    "            if ic_deriv_spec:\n",
    "                # Ensure ic_points_dict tensors used for derivatives have requires_grad=True\n",
    "                grad_enabled_ic_points_dict = {\n",
    "                    k: v.clone().detach().requires_grad_(True) for k,v in ic_points_dict.items()\n",
    "                }\n",
    "                grad_enabled_ic_model_input = self._prepare_model_input(grad_enabled_ic_points_dict)\n",
    "                model_outputs_ic_for_deriv = self.model(grad_enabled_ic_model_input)\n",
    "\n",
    "                derivatives_at_ic = self._compute_derivatives(grad_enabled_ic_points_dict, model_outputs_ic_for_deriv)\n",
    "\n",
    "            loss_ic = self.pde_problem.initial_conditions(ic_points_dict, model_outputs_ic, derivatives_at_ic, self.model, kappa_value)\n",
    "\n",
    "        total_loss = (loss_weights['pde'] * loss_pde +\n",
    "                      loss_weights['bc'] * loss_bc +\n",
    "                      loss_weights['ic'] * loss_ic)\n",
    "        total_loss.backward()\n",
    "        self._current_losses = {'pde': loss_pde.item(), 'bc': loss_bc.item(),\n",
    "                                'ic': loss_ic.item(), 'total': total_loss.item()}\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, num_epochs, kappa_value,\n",
    "              num_collocation_pts, num_bc_pts_per_face, num_ic_pts, # Renamed for clarity\n",
    "              collocation_strategy='uniform',\n",
    "              loss_weights={'pde': 1.0, 'bc': 1.0, 'ic': 1.0},\n",
    "              log_epochs=[0, 1000, 5000, 10000],\n",
    "              num_test_pts_error_grid=1001):\n",
    "\n",
    "        cumulative_time_s = 0.0\n",
    "        self.epoch_wise_log = []\n",
    "\n",
    "        for epoch in range(num_epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Common point sampling (moved outside optimizer-specific block)\n",
    "            # These return dicts like {'x': tensor, 't': tensor}\n",
    "            collocation_points_dict = self.pde_problem.get_collocation_points(\n",
    "                num_collocation_pts, kappa_value, self.device, collocation_strategy\n",
    "            )\n",
    "\n",
    "            bc_points_dict = self.pde_problem.get_boundary_points_general(\n",
    "                num_bc_pts_per_face, kappa_value, self.device, strategy=collocation_strategy # Use same strategy for BCs\n",
    "            )\n",
    "            if bc_points_dict is None: # Fallback to hyperrect if general not implemented\n",
    "                bc_points_dict = self.pde_problem.get_boundary_points_hyperrect(\n",
    "                    num_bc_pts_per_face, kappa_value, self.device, strategy=collocation_strategy\n",
    "                )\n",
    "\n",
    "            ic_points_dict = {}\n",
    "            if self.pde_problem.time_dependent:\n",
    "                ic_points_dict = self.pde_problem.get_initial_points(\n",
    "                    num_ic_pts, kappa_value, self.device, strategy=collocation_strategy # Use same strategy for ICs\n",
    "                )\n",
    "\n",
    "            if self.optimizer_str == \"adam\":\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # PDE Loss\n",
    "                colloc_model_input = self._prepare_model_input(collocation_points_dict)\n",
    "                model_outputs_colloc = self.model(colloc_model_input)\n",
    "                pde_deriv_spec = self.pde_problem.get_required_derivative_orders()\n",
    "                derivatives_colloc = self._compute_derivatives(collocation_points_dict, model_outputs_colloc, pde_deriv_spec)\n",
    "                pde_res = self.pde_problem.pde_residual(collocation_points_dict, model_outputs_colloc, derivatives_colloc, kappa_value)\n",
    "                loss_pde = torch.mean(pde_res**2)\n",
    "\n",
    "                loss_bc = torch.tensor(0.0, device=self.device)\n",
    "                if bc_points_dict: # Check if not empty dictionary from get_boundary_points\n",
    "                    # Check if the Tensors within the dict are empty\n",
    "                    if all(tensor.numel() > 0 for tensor in bc_points_dict.values()):\n",
    "                        bc_model_input = self._prepare_model_input(bc_points_dict)\n",
    "                        model_outputs_bc = self.model(bc_model_input)\n",
    "                        derivatives_at_bc = {}\n",
    "                        bc_deriv_spec = self.pde_problem.get_required_derivative_orders_for_bc()\n",
    "                        if bc_deriv_spec:\n",
    "                            grad_enabled_bc_points_dict = {\n",
    "                                k: v.clone().detach().requires_grad_(True) for k,v in bc_points_dict.items()\n",
    "                            }\n",
    "                            grad_enabled_bc_model_input = self._prepare_model_input(grad_enabled_bc_points_dict)\n",
    "                            # Need to re-evaluate model on grad-enabled inputs to build graph for derivatives\n",
    "                            model_outputs_bc_for_deriv = self.model(grad_enabled_bc_model_input)\n",
    "                            derivatives_at_bc = self._compute_derivatives(grad_enabled_bc_points_dict, model_outputs_bc_for_deriv, bc_deriv_spec)\n",
    "\n",
    "                        loss_bc = self.pde_problem.boundary_conditions(\n",
    "                            inputs_bc=bc_points_dict,\n",
    "                            model_outputs_bc=model_outputs_bc, # This is from original non-grad points\n",
    "                            derivatives_bc=derivatives_at_bc,  # But derivatives from grad-enabled\n",
    "                            model=self.model,\n",
    "                            kappa_value=kappa_value\n",
    "                        )\n",
    "                    else: # Handle case where bc_points_dict is not empty but contains empty tensors\n",
    "                        loss_bc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "\n",
    "                # IC Loss\n",
    "                loss_ic = torch.tensor(0.0, device=self.device)\n",
    "                if self.pde_problem.time_dependent and ic_points_dict:\n",
    "                    if all(tensor.numel() > 0 for tensor in ic_points_dict.values()):\n",
    "                        ic_model_input = self._prepare_model_input(ic_points_dict)\n",
    "                        model_outputs_ic = self.model(ic_model_input)\n",
    "                        derivatives_at_ic = {}\n",
    "                        ic_deriv_spec = self.pde_problem.get_required_derivative_orders_for_ic()\n",
    "                        if ic_deriv_spec:\n",
    "                            grad_enabled_ic_points_dict = {\n",
    "                                k: v.clone().detach().requires_grad_(True) for k,v in ic_points_dict.items()\n",
    "                            }\n",
    "                            grad_enabled_ic_model_input = self._prepare_model_input(grad_enabled_ic_points_dict)\n",
    "                            model_outputs_ic_for_deriv = self.model(grad_enabled_ic_model_input)\n",
    "                            derivatives_at_ic = self._compute_derivatives(grad_enabled_ic_points_dict, model_outputs_ic_for_deriv, ic_deriv_spec)\n",
    "\n",
    "                        loss_ic = self.pde_problem.initial_conditions(\n",
    "                            inputs_ic=ic_points_dict,\n",
    "                            model_outputs_ic=model_outputs_ic,\n",
    "                            derivatives_ic=derivatives_at_ic,\n",
    "                            model=self.model,\n",
    "                            kappa_value=kappa_value\n",
    "                        )\n",
    "                    else: # Handle case where ic_points_dict is not empty but contains empty tensors\n",
    "                        loss_ic = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "\n",
    "                total_loss = (loss_weights['pde'] * loss_pde +\n",
    "                              loss_weights['bc'] * loss_bc +\n",
    "                              loss_weights['ic'] * loss_ic)\n",
    "\n",
    "                if epoch > 0:\n",
    "                    total_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                current_total_loss = total_loss.item()\n",
    "                current_pde_loss = loss_pde.item()\n",
    "                current_bc_loss = loss_bc.item()\n",
    "                current_ic_loss = loss_ic.item()\n",
    "\n",
    "            elif self.optimizer_str == \"lbfgs\":\n",
    "                if epoch > 0:\n",
    "                    self.model.train()\n",
    "                    self.optimizer.step(lambda: self._closure_lbfgs(\n",
    "                        collocation_points_dict, bc_points_dict, ic_points_dict, kappa_value, loss_weights\n",
    "                    ))\n",
    "                # For LBFGS, losses are updated within the closure\n",
    "                current_total_loss = self._current_losses.get('total', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "                current_pde_loss = self._current_losses.get('pde', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "                current_bc_loss = self._current_losses.get('bc', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "                current_ic_loss = self._current_losses.get('ic', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "\n",
    "            # ... (rest of logging logic is good) ...\n",
    "            epoch_duration_s = time.time() - epoch_start_time\n",
    "            if epoch > 0 : cumulative_time_s += epoch_duration_s\n",
    "\n",
    "            if epoch in log_epochs or epoch == num_epochs:\n",
    "                grad_norm = 0.0\n",
    "                if epoch > 0:\n",
    "                    for p in self.model.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            grad_norm += p.grad.detach().data.norm(2).item() ** 2\n",
    "                    grad_norm = grad_norm ** 0.5 if grad_norm > 0 else 0.0\n",
    "\n",
    "                error_metrics_on_grid = self._calculate_error_metrics_on_test_grid(kappa_value, num_test_pts_error_grid)\n",
    "\n",
    "                # Track L2 norm of weights for regularization and diagnostics\n",
    "                l2_norm_weights = 0.0\n",
    "                for param in self.model.parameters():\n",
    "                    if param.requires_grad: # Usually all model parameters do\n",
    "                        l2_norm_weights += torch.linalg.norm(param.data).item()**2\n",
    "                l2_norm_weights = np.sqrt(l2_norm_weights) if l2_norm_weights > 0 else 0.0\n",
    "\n",
    "                gpu_mem_peak_mb = float('nan')\n",
    "                if self.device.type == 'cuda':\n",
    "                    # Peak memory allocated on this device since the last reset\n",
    "                    gpu_mem_peak_mb = torch.cuda.max_memory_allocated(self.device) / (1024**2) # Convert to MB\n",
    "                    torch.cuda.reset_peak_memory_stats(self.device) # Reset for the next interval\n",
    "\n",
    "                log_entry = {\n",
    "                    'epoch': epoch, 'time_s': cumulative_time_s,\n",
    "                    'loss_total': current_total_loss, 'loss_pde': current_pde_loss,\n",
    "                    'loss_bc': current_bc_loss, 'loss_ic': current_ic_loss,\n",
    "                    'grad_norm_l2': grad_norm, 'l2_norm_weights': l2_norm_weights,\n",
    "                    'gpu_mem_peak_mb': gpu_mem_peak_mb,\n",
    "                }\n",
    "                log_entry.update(error_metrics_on_grid)\n",
    "                self.epoch_wise_log.append(log_entry)\n",
    "\n",
    "                print(f\"Epoch {epoch}/{num_epochs}, Loss: {current_total_loss:.3e}, \"\n",
    "                      f\"L2_err_rel: {log_entry.get('L2_err_rel', float('nan')):.3e}, GradNorm: {grad_norm:.3e}\")\n",
    "\n",
    "        print(f\"Training finished. Total active time: {cumulative_time_s:.2f}s\")\n",
    "        return self.epoch_wise_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbc55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict, field\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # Identification\n",
    "    pde_name: str\n",
    "    kappa_val: float\n",
    "    activation_str: str\n",
    "    seed: int\n",
    "\n",
    "    # Architecture\n",
    "    depth: int # Number of hidden layers\n",
    "    width: int # Neurons per hidden layer\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_type: str\n",
    "    lr: float\n",
    "\n",
    "    # Training\n",
    "    epochs: int\n",
    "\n",
    "    # Logging & Error Evaluation\n",
    "    log_epochs_list: list = field(default_factory=lambda: [x for x in range(0, 10001, 100)])\n",
    "    num_test_pts_error_grid: int = 1001\n",
    "\n",
    "    # Loss Weights\n",
    "    loss_weight_pde: float = 1.0\n",
    "    loss_weight_bc: float = 1.0\n",
    "    loss_weight_ic: float = 1.0\n",
    "\n",
    "    # Collocation (points takes precedence over factor)\n",
    "    M_collocation_pts: int = field(default=None)\n",
    "    M_collocation_factor: int = field(default=10)\n",
    "\n",
    "    # IC/BC Points\n",
    "    num_total_bc_pts: Optional[int] = field(default=None) # Total BC points across all faces\n",
    "    num_bc_pts_per_face: Optional[int] = field(default=None) # If not specified, heuristic will be used\n",
    "    num_ic_pts: Optional[int] = field(default=None) # Only for time-dependent PDEs\n",
    "    collocation_scheme: str = field(default='uniform')\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, base_results_dir=\"data/\", pde_map=None, device='cpu'):\n",
    "        self.base_results_dir = base_results_dir\n",
    "        self.pde_map = pde_map if pde_map is not None else {}\n",
    "        self.device = device\n",
    "        os.makedirs(self.base_results_dir, exist_ok=True)\n",
    "\n",
    "    def _get_run_dir(self, config: ExperimentConfig):\n",
    "        # Format kappa_val for filename safety, e.g., replace decimal point\n",
    "        kappa_str = f\"{config.kappa_val:.1e}\".replace('.', 'p').replace('+', '') # e.g. 1p0e-03\n",
    "\n",
    "        run_path = os.path.join(\n",
    "            self.base_results_dir,\n",
    "            config.pde_name,\n",
    "            f\"kappa_{kappa_str}\",\n",
    "            f\"act_{config.activation_str}\",\n",
    "            f\"N_{config.width}\",\n",
    "            f\"D_{config.depth}\",\n",
    "            f\"seed_{config.seed}\"\n",
    "        )\n",
    "        os.makedirs(run_path, exist_ok=True)\n",
    "        return run_path\n",
    "\n",
    "    def run_single_experiment(self, config: ExperimentConfig):\n",
    "        pde_instance = self.pde_map.get(config.pde_name)\n",
    "        if pde_instance is None:\n",
    "            print(f\"Error: PDE problem '{config.pde_name}' not found in pde_map.\")\n",
    "            return\n",
    "\n",
    "        run_dir = self._get_run_dir(config)\n",
    "        print(f\"\\n--- Running Experiment: {run_dir} ---\")\n",
    "        print(f\"Config: {config}\")\n",
    "\n",
    "        with open(os.path.join(run_dir, \"config.json\"), 'w') as f:\n",
    "            json.dump(asdict(config), f, indent=2)\n",
    "\n",
    "        torch.manual_seed(config.seed)\n",
    "        np.random.seed(config.seed)\n",
    "\n",
    "        # Determine model input_dim based on pde_instance.input_vars\n",
    "        # Could also use pde_instance.spatial_domain_dim + (1 if pde_instance.time_dependent else 0)\n",
    "        model_input_dim = len(pde_instance.input_vars)\n",
    "\n",
    "        model = create_pinn_model(\n",
    "            input_dim=model_input_dim,\n",
    "            output_dim=pde_instance.output_dim,\n",
    "            n_hidden_layers=config.depth,\n",
    "            n_neurons_per_layer=config.width,\n",
    "            activation_str=config.activation_str,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(model, pde_instance,\n",
    "                          optimizer_str=config.optimizer_type,\n",
    "                          learning_rate=config.lr,\n",
    "                          device=self.device)\n",
    "\n",
    "\n",
    "        # Collocation points:\n",
    "        if config.M_collocation_pts is not None:\n",
    "            M_collocation = config.M_collocation_pts\n",
    "        else:\n",
    "            # Specified or default collocation factor if direct points not given\n",
    "            M_collocation = config.width * config.M_collocation_factor\n",
    "\n",
    "        # For BC points:\n",
    "        actual_num_bc_pts_per_face = 0\n",
    "        if config.num_total_bc_pts is not None:\n",
    "            num_spatial_dims = pde_instance.spatial_domain_dim\n",
    "            num_faces = 2 * num_spatial_dims if num_spatial_dims > 0 else 0\n",
    "            if num_faces > 0:\n",
    "                actual_num_bc_pts_per_face = config.num_total_bc_pts // num_faces\n",
    "            else: # If no spatial dims, num_total_bc_pts should ideally be 0 or ignored\n",
    "                actual_num_bc_pts_per_face = 0 # or handle appropriately\n",
    "        elif config.num_bc_pts_per_face is not None:\n",
    "            actual_num_bc_pts_per_face = config.num_bc_pts_per_face\n",
    "        else: # Fallback or default heuristic if not specified\n",
    "            num_spatial_dims = pde_instance.spatial_domain_dim\n",
    "            num_faces = 2 * num_spatial_dims if num_spatial_dims > 0 else 0\n",
    "            if num_faces > 0:\n",
    "                heuristic_bc_factor = 20\n",
    "                actual_num_bc_pts_per_face = M_collocation // (heuristic_bc_factor * num_faces)\n",
    "                actual_num_bc_pts_per_face = max(10, actual_num_bc_pts_per_face) # Min points\n",
    "            else:\n",
    "                actual_num_bc_pts_per_face = 0\n",
    "\n",
    "        # For IC points:\n",
    "        actual_num_ic_pts = 0\n",
    "        if pde_instance.time_dependent:\n",
    "            if config.num_ic_pts is not None:\n",
    "                actual_num_ic_pts = config.num_ic_pts\n",
    "            else: # Fallback or default heuristic\n",
    "                heuristic_ic_factor = 10\n",
    "                actual_num_ic_pts = M_collocation // heuristic_ic_factor\n",
    "                actual_num_ic_pts = max(10, actual_num_ic_pts) # Min points\n",
    "\n",
    "        # Adjust log_epochs based on actual config.epochs\n",
    "        actual_log_epochs = [e for e in config.log_epochs_list if e <= config.epochs]\n",
    "        if config.epochs not in actual_log_epochs:\n",
    "            actual_log_epochs.append(config.epochs)\n",
    "        actual_log_epochs = sorted(list(set(actual_log_epochs)))\n",
    "        if 0 not in actual_log_epochs : actual_log_epochs.insert(0,0)\n",
    "\n",
    "\n",
    "        epoch_wise_log_data = trainer.train(\n",
    "            num_epochs=config.epochs,\n",
    "            kappa_value=config.kappa_val,\n",
    "            num_collocation_pts=M_collocation,\n",
    "            num_bc_pts_per_face=actual_num_bc_pts_per_face,\n",
    "            num_ic_pts=actual_num_ic_pts,\n",
    "            collocation_strategy=config.collocation_scheme,\n",
    "            log_epochs=actual_log_epochs,\n",
    "            num_test_pts_error_grid=config.num_test_pts_error_grid\n",
    "        )\n",
    "\n",
    "        df_epoch_log = pd.DataFrame(epoch_wise_log_data)\n",
    "        df_epoch_log.to_csv(os.path.join(run_dir, \"training_log.csv\"), index=False)\n",
    "\n",
    "        final_metrics = {}\n",
    "        if not df_epoch_log.empty:\n",
    "            last_epoch_data = df_epoch_log.iloc[-1]\n",
    "            final_metrics = {\n",
    "                key: last_epoch_data.get(key, float('nan'))\n",
    "                for key in ['time_s', 'loss_total', 'L1_err_rel', 'L2_err_rel',\n",
    "                            'Linf_err_rel', 'PDE_residual_max', 'grad_norm_l2']\n",
    "            }\n",
    "\n",
    "        summary_data = {\"final_metrics\": final_metrics, \"fit_results\": {}} # Config saved separately\n",
    "        with open(os.path.join(run_dir, \"summary.json\"), 'w') as f:\n",
    "            json.dump(summary_data, f, indent=2, cls=NpEncoder) # Handle numpy types if any\n",
    "\n",
    "        print(f\"Finished experiment. Final L2_err_rel: {final_metrics.get('L2_err_rel', 'N/A'):.3e}\")\n",
    "\n",
    "# Helper for JSON serialization if numpy types are used in summary\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186fc74",
   "metadata": {},
   "source": [
    "## Concerete PDEProblem Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d45a2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialLinearPDE(PDEProblem):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"TrivialLinear\",\n",
    "                         input_vars=['x'],\n",
    "                         output_vars=['u'],\n",
    "                         kappa_name=\"N/A\",\n",
    "                         default_kappa_value=1.0)\n",
    "\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        return {'x': (0.0, 1.0)}\n",
    "\n",
    "    def pde_residual(self, inputs: Dict[str, torch.Tensor],\n",
    "                     model_outputs: torch.Tensor,\n",
    "                     derivatives: Dict[str, torch.Tensor],\n",
    "                     kappa_value: float) -> torch.Tensor:\n",
    "        # PDE: -u_xx = 0  => u_xx = 0. model_outputs is u(x)\n",
    "        return derivatives['d2(u)_dx(2)'] # Residual is u_xx\n",
    "\n",
    "    def boundary_conditions(self, inputs_bc: Dict[str, torch.Tensor],\n",
    "                            model_outputs_bc: torch.Tensor,\n",
    "                            derivatives_bc: Dict[str, torch.Tensor], # Added\n",
    "                            model: nn.Module, # Added model for API consistency\n",
    "                            kappa_value: float) -> torch.Tensor:\n",
    "        # BC: u(0)=0, u(1)=1. model_outputs_bc is NN(x_bc) -> predicted u at boundary\n",
    "        x_vals = inputs_bc['x'].squeeze()\n",
    "        loss = torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "\n",
    "        # u(0) = 0\n",
    "        u_at_0_pred = model_outputs_bc[x_vals == self.get_domain_bounds()['x'][0]]\n",
    "        if u_at_0_pred.numel() > 0:\n",
    "            loss += torch.mean((u_at_0_pred - 0.0)**2)\n",
    "\n",
    "        # u(1) = 1\n",
    "        u_at_1_pred = model_outputs_bc[x_vals == self.get_domain_bounds()['x'][1]]\n",
    "        if u_at_1_pred.numel() > 0:\n",
    "            loss += torch.mean((u_at_1_pred - 1.0)**2)\n",
    "        return loss\n",
    "\n",
    "    def get_ground_truth(self, inputs: Dict[str, torch.Tensor],\n",
    "                         kappa_value: float) -> Optional[torch.Tensor]:\n",
    "        # Solution: u(x) = x\n",
    "        x = inputs['x']\n",
    "        return x.clone()\n",
    "\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        return {\n",
    "            'u': {\n",
    "                ('x',): 2  # We need up to d2u/dx2\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PoissonPDE(PDEProblem):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Poisson\",\n",
    "                         input_vars=['x'],\n",
    "                         output_vars=['u'],\n",
    "                         kappa_name=\"N/A\",\n",
    "                         default_kappa_value=1.0)\n",
    "        self.forcing_fn_torch = lambda x_tensor: torch.sin(torch.pi * x_tensor)\n",
    "        self.analytical_sol_np = lambda x_np: (1.0 / (np.pi**2)) * np.sin(np.pi * x_np.squeeze(-1) if x_np.ndim > 1 else np.pi * x_np)\n",
    "\n",
    "\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        return {'x': (0.0, 1.0)}\n",
    "\n",
    "    def pde_residual(self, inputs: Dict[str, torch.Tensor],\n",
    "                     model_outputs: torch.Tensor,\n",
    "                     derivatives: Dict[str, torch.Tensor],\n",
    "                     kappa_value: float) -> torch.Tensor:\n",
    "        # PDE: -u_xx = sin(pi*x)  =>  u_xx + sin(pi*x) = 0\n",
    "        x = inputs['x']\n",
    "        # model_outputs is u(x), derivatives['d2(u)_dx(2)'] is u_xx\n",
    "        u_xx = derivatives['d2(u)_dx(2)']\n",
    "        f_x = self.forcing_fn_torch(x)\n",
    "        return u_xx + f_x\n",
    "\n",
    "    def boundary_conditions(self, inputs_bc: Dict[str, torch.Tensor],\n",
    "                            model_outputs_bc: torch.Tensor,\n",
    "                            derivatives_bc: Dict[str, torch.Tensor], # Added\n",
    "                            model: nn.Module,\n",
    "                            kappa_value: float) -> torch.Tensor:\n",
    "        # BC: u(0)=0, u(1)=0\n",
    "        x_vals = inputs_bc['x'].squeeze()\n",
    "        loss = torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "\n",
    "        u_at_0_pred = model_outputs_bc[x_vals == self.get_domain_bounds()['x'][0]]\n",
    "        if u_at_0_pred.numel() > 0:\n",
    "            loss += torch.mean((u_at_0_pred - 0.0)**2)\n",
    "\n",
    "        u_at_1_pred = model_outputs_bc[x_vals == self.get_domain_bounds()['x'][1]]\n",
    "        if u_at_1_pred.numel() > 0:\n",
    "            loss += torch.mean((u_at_1_pred - 0.0)**2)\n",
    "        return loss\n",
    "\n",
    "    def get_ground_truth(self, inputs: Dict[str, torch.Tensor],\n",
    "                         kappa_value: float) -> Optional[torch.Tensor]:\n",
    "        x_tensor = inputs['x']\n",
    "        x_np = x_tensor.detach().cpu().numpy()\n",
    "        u_true_np = self.analytical_sol_np(x_np)\n",
    "        return torch.tensor(u_true_np, dtype=torch.float32, device=x_tensor.device).reshape_as(x_tensor)\n",
    "\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        return {\n",
    "            'u': {\n",
    "                ('x',): 2\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurgersPDE(PDEProblem):\n",
    "    def __init__(self, periodic_bc_scheme: str = \"soft_constraint\"):\n",
    "        super().__init__(name=\"Burgers\",\n",
    "                         input_vars=['t', 'x'],\n",
    "                         output_vars=['u'],\n",
    "                         time_var='t',\n",
    "                         kappa_name=\"1/nu\",\n",
    "                         default_kappa_value=100.0) # Default nu = 0.01\n",
    "        self.periodic_bc_scheme = periodic_bc_scheme # \"soft_constraint\" or \"hard_transform\" (more complex)\n",
    "        # Ground truth might be complex, consider a helper or precomputed data\n",
    "        self.ground_truth_solver = None # Placeholder for a more sophisticated solver\n",
    "\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        return {'t': (0.0, 1.0), 'x': (-1.0, 1.0)} # Common domain\n",
    "\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        return {\n",
    "            'u': {\n",
    "                ('t',): 1,  # For u_t\n",
    "                ('x',): 2   # For u_x and u_xx\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def pde_residual(self, inputs: Dict[str, torch.Tensor],\n",
    "                     model_outputs: torch.Tensor,\n",
    "                     derivatives: Dict[str, torch.Tensor],\n",
    "                     kappa_value: float) -> torch.Tensor:\n",
    "        # PDE: u_t + u * u_x - nu * u_xx = 0\n",
    "        # model_outputs is u(t,x) (shape: batch, 1)\n",
    "        # derivatives: 'd1(u)_dt(1)', 'd1(u)_dx(1)', 'd2(u)_dx(2)'\n",
    "        nu = 1.0 / kappa_value\n",
    "        u = model_outputs[:, 0:1] # Ensure it's (batch, 1)\n",
    "\n",
    "        u_t = derivatives['d1(u)_dt(1)']\n",
    "        u_x = derivatives['d1(u)_dx(1)']\n",
    "        u_xx = derivatives['d2(u)_dx(2)']\n",
    "\n",
    "        residual = u_t + u * u_x - nu * u_xx\n",
    "        return residual # Shape (batch, 1)\n",
    "\n",
    "    def initial_conditions(self, inputs_ic: Dict[str, torch.Tensor],\n",
    "                           model_outputs_ic: torch.Tensor,\n",
    "                           model: nn.Module,\n",
    "                            derivatives_ic: Dict[str, torch.Tensor],\n",
    "                           kappa_value: float) -> torch.Tensor:\n",
    "        # IC: u(0, x) = -sin(pi*x)\n",
    "        # inputs_ic will have t=0 and varying x\n",
    "        # model_outputs_ic are the PINN predictions u(0,x)_pred (shape: batch, 1)\n",
    "        x_ic = inputs_ic['x']\n",
    "        u_true_ic = -torch.sin(torch.pi * x_ic)\n",
    "\n",
    "        loss_ic = torch.mean((model_outputs_ic - u_true_ic)**2)\n",
    "        return loss_ic\n",
    "\n",
    "    def boundary_conditions(self, inputs_bc: Dict[str, torch.Tensor],\n",
    "                        model_outputs_bc: torch.Tensor, # Output on all points from get_boundary_points_hyperrect\n",
    "                        derivatives_bc: Dict[str, torch.Tensor], # Derivatives on all those points\n",
    "                        model: nn.Module, # Keep for flexibility\n",
    "                        kappa_value: float) -> torch.Tensor:\n",
    "        if self.periodic_bc_scheme == \"soft_constraint\":\n",
    "            t_b = inputs_bc[self.time_var] # (N_total_bc_pts, 1)\n",
    "            x_b = inputs_bc['x']           # (N_total_bc_pts, 1)\n",
    "\n",
    "            x_min_val, x_max_val = self.get_domain_bounds()['x']\n",
    "\n",
    "            # Indices for points at x_min and x_max\n",
    "            # This assumes get_boundary_points_hyperrect samples t values consistently for both faces\n",
    "            # A robust way to ensure pairing:\n",
    "            # Find all unique t values that appear on BOTH x_min and x_max boundaries\n",
    "\n",
    "            # Simplified pairing assuming structure from get_boundary_points_hyperrect:\n",
    "            # (num_points_per_face for x_min, then num_points_per_face for x_max, with matching t's)\n",
    "            # This relies on num_points_per_face being the same for both faces.\n",
    "            # And t values being sampled identically.\n",
    "\n",
    "            num_pts_on_one_face = x_b[x_b == x_min_val].shape[0]\n",
    "\n",
    "            # Assuming t_coords are sampled consistently for x_min and x_max faces\n",
    "            # and are stacked, e.g., [t_face1, t_face2, ...]\n",
    "            # More careful selection/pairing might be needed if sampling strategy is complex.\n",
    "\n",
    "            # A more direct way for soft periodic BC:\n",
    "            # Generate N_periodic_t time points *within this function*.\n",
    "            # This bypasses the need to perfectly align points from get_boundary_points_hyperrect.\n",
    "\n",
    "            domain_b = self.get_domain_bounds()\n",
    "            t_min_b, t_max_b = domain_b[self.time_var]\n",
    "            x_min_b, x_max_b = domain_b['x']\n",
    "\n",
    "            # Use a portion of the provided BC points to define num_periodic_pts\n",
    "            # num_periodic_pts = model_outputs_bc.shape[0] // 2 # Assuming half on x_min, half on x_max\n",
    "            # Or a fixed number, e.g., from config.num_bc_pts_per_face\n",
    "            num_periodic_pts = inputs_bc[self.time_var][inputs_bc['x'] == x_min_b].shape[0]\n",
    "            if num_periodic_pts == 0: return torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "\n",
    "\n",
    "            t_periodic = torch.rand(num_periodic_pts, 1, device=model_outputs_bc.device) * (t_max_b - t_min_b) + t_min_b\n",
    "            t_periodic.requires_grad_(True) # In case t-derivatives were needed at BC\n",
    "\n",
    "            x_at_min_periodic = torch.full_like(t_periodic, x_min_b).requires_grad_(True)\n",
    "            x_at_max_periodic = torch.full_like(t_periodic, x_max_b).requires_grad_(True)\n",
    "\n",
    "            # Prepare inputs for model\n",
    "            inputs_at_xmin_dict = {self.time_var: t_periodic, 'x': x_at_min_periodic}\n",
    "            inputs_at_xmax_dict = {self.time_var: t_periodic, 'x': x_at_max_periodic}\n",
    "\n",
    "            model_input_xmin = torch.cat([inputs_at_xmin_dict[var] for var in self.input_vars], dim=1)\n",
    "            model_input_xmax = torch.cat([inputs_at_xmax_dict[var] for var in self.input_vars], dim=1)\n",
    "\n",
    "            u_at_xmin = model(model_input_xmin)\n",
    "            u_at_xmax = model(model_input_xmax)\n",
    "            loss_val_periodic = torch.mean((u_at_xmin - u_at_xmax)**2)\n",
    "\n",
    "            loss_deriv_periodic = torch.tensor(0.0, device=loss_val_periodic.device)\n",
    "            # For derivative u_x(t, x_min) = u_x(t, x_max)\n",
    "            # We need to compute derivatives w.r.t x at these specific (t_periodic, x_at_min/max_periodic) points\n",
    "\n",
    "            # This uses the Trainer's _compute_derivatives, which needs the Trainer instance.\n",
    "            # This is a bit of a circular dependency if not handled carefully.\n",
    "            # Alternative: Compute grads directly here.\n",
    "\n",
    "            # Compute u_x at x_min for t_periodic\n",
    "            du_dx_at_xmin = torch.autograd.grad(u_at_xmin, x_at_min_periodic, grad_outputs=torch.ones_like(u_at_xmin), create_graph=True)[0]\n",
    "            # Compute u_x at x_max for t_periodic\n",
    "            du_dx_at_xmax = torch.autograd.grad(u_at_xmax, x_at_max_periodic, grad_outputs=torch.ones_like(u_at_xmax), create_graph=True)[0]\n",
    "\n",
    "            if du_dx_at_xmin is not None and du_dx_at_xmax is not None:\n",
    "                loss_deriv_periodic = torch.mean((du_dx_at_xmin - du_dx_at_xmax)**2)\n",
    "            else:\n",
    "                print(\"Warning: Could not compute derivatives for periodic BC enforcement.\")\n",
    "\n",
    "            return loss_val_periodic + loss_deriv_periodic # Add relative weighting if desired\n",
    "\n",
    "        elif self.periodic_bc_scheme == \"hard_transform\":\n",
    "            raise NotImplementedError(\"Hard periodic BC transform not implemented here.\")\n",
    "        return torch.tensor(0.0, device=model_outputs_bc.device) # Default if no scheme matches\n",
    "\n",
    "    def get_required_derivative_orders_for_bc(self) -> Optional[Dict[str, Dict[Tuple[str, ...], int]]]:\n",
    "        if self.periodic_bc_scheme == \"soft_constraint\":\n",
    "            return {'u': {('x',): 1}} # Need u_x for periodic u_x\n",
    "        return None\n",
    "\n",
    "    @cache\n",
    "    def _compute_theta_0_coeffs(self, nu: float, N_terms_fourier: int,\n",
    "                             N_points_integrate: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes Fourier coefficients c_n(0) for theta_0(x) for n = 0, 1, ..., N_terms_fourier.\n",
    "        theta_0(x) = exp( (1 - cos(pi*x)) / (2*nu*pi) )\n",
    "        c_n(0) = (1/L) * integral_{-L/2}^{L/2} theta_0(x) * exp(-i*2*pi*n*x/L) dx, with L=2.\n",
    "            = 0.5 * integral_{-1}^{1} theta_0(x) * exp(-i*n*pi*x) dx\n",
    "        Since theta_0(x) is real and even, c_n are real and c_n = c_{-n}.\n",
    "        So, c_n(0) = 0.5 * integral_{-1}^{1} theta_0(x) * cos(n*pi*x) dx.\n",
    "        We return an array [c_0(0), c_1(0), ..., c_N_terms_fourier(0)].\n",
    "        \"\"\"\n",
    "        xi = torch.linspace(-1.0, 1.0, N_points_integrate, dtype=torch.float32, device=device)\n",
    "\n",
    "        # theta_0(x) = exp( (1 - cos(pi*x)) / (2*nu*pi) )  <-- Corrected sign from original derivation\n",
    "        factor = 1.0 / (2.0 * nu * torch.pi)\n",
    "        theta_0_on_xi = torch.exp(factor * (1.0 - torch.cos(torch.pi * xi)))\n",
    "\n",
    "        c_n_0_array = torch.zeros(N_terms_fourier + 1, dtype=torch.float32, device=device)\n",
    "\n",
    "        for n_val in range(N_terms_fourier + 1): # n = 0, 1, ..., N_terms_fourier\n",
    "            integrand = theta_0_on_xi * torch.cos(n_val * torch.pi * xi)\n",
    "            c_n_0_array[n_val] = 0.5 * torch.trapezoid(y=integrand, x=xi) # type: ignore\n",
    "\n",
    "        return c_n_0_array\n",
    "\n",
    "    def get_ground_truth(self, inputs: Dict[str, torch.Tensor],\n",
    "                       kappa_value: float) -> Optional[torch.Tensor]:\n",
    "        x_eval = inputs['x']\n",
    "        t_eval = inputs['t']\n",
    "        current_device = x_eval.device\n",
    "\n",
    "        if kappa_value == 0: # Avoid division by zero if nu is kappa\n",
    "            nu = float('inf') # effectively no viscosity, though formula breaks down\n",
    "            # Handle inviscid case separately if needed, or raise error\n",
    "            print(\"Warning: kappa_value is 0, results for Burgers' GT might be ill-defined with this formula.\")\n",
    "            # For inviscid Burgers with u0 = -sin(pi*x), shock forms. This solution method is for viscous.\n",
    "            return torch.full_like(x_eval, float('nan'))\n",
    "\n",
    "        nu = 1.0 / kappa_value\n",
    "\n",
    "        N_terms_fourier = getattr(self, 'N_terms_fourier_gt', 75) # Can be tuned, make it an attribute\n",
    "        N_points_integrate_coeffs = getattr(self, 'N_points_integrate_coeffs_gt', 4096)\n",
    "\n",
    "        # --- (Optional) Caching logic for c_n_0_coeffs ---\n",
    "        # cache_key = (float(nu), N_terms_fourier, N_points_integrate_coeffs, str(current_device))\n",
    "        # if hasattr(self, '_cached_theta_coeffs') and cache_key in self._cached_theta_coeffs:\n",
    "        #    c_n_0_real = self._cached_theta_coeffs[cache_key]\n",
    "        # else:\n",
    "        #    c_n_0_real = self._compute_theta_0_coeffs(nu, N_terms_fourier, N_points_integrate_coeffs, device=current_device)\n",
    "        #    if not hasattr(self, '_cached_theta_coeffs'): self._cached_theta_coeffs = {}\n",
    "        #    self._cached_theta_coeffs[cache_key] = c_n_0_real\n",
    "        # For simplicity now, recompute:\n",
    "        c_n_0_real = self._compute_theta_0_coeffs(nu, N_terms_fourier, N_points_integrate_coeffs, device=current_device)\n",
    "\n",
    "        x = x_eval.view(-1, 1)\n",
    "        t = t_eval.view(-1, 1)\n",
    "\n",
    "        n_vals = torch.arange(0, N_terms_fourier + 1, device=current_device, dtype=torch.float32).view(1, -1)\n",
    "        c_n_coeffs_for_sum = c_n_0_real.view(1, -1)\n",
    "\n",
    "        # Theta sum: theta(x,t) = c_0(0) + Sum_{n=1 to N} 2 * c_n(0) * cos(n*pi*x) * exp(-nu*(n*pi)^2*t)\n",
    "        # Theta_x sum: theta_x(x,t) = Sum_{n=1 to N} 2 * c_n(0) * (-n*pi*sin(n*pi*x)) * exp(-nu*(n*pi)^2*t)\n",
    "\n",
    "        # n=0 term for theta\n",
    "        theta_val = c_n_coeffs_for_sum[:, 0:1] * torch.ones_like(x) # Term for c_0\n",
    "        theta_x_val = torch.zeros_like(x) # n=0 term for theta_x is zero\n",
    "\n",
    "        # n > 0 terms\n",
    "        n_pos_vals = n_vals[:, 1:]  # Shape (1, N_terms_fourier)\n",
    "        c_n_pos_coeffs = c_n_coeffs_for_sum[:, 1:] # Shape (1, N_terms_fourier)\n",
    "\n",
    "        # Common factors for n > 0\n",
    "        n_pi_x = n_pos_vals * torch.pi * x  # Shape (B, N_terms_fourier) via broadcasting\n",
    "        exp_decay = torch.exp(-nu * (n_pos_vals * torch.pi)**2 * t) # Shape (B, N_terms_fourier)\n",
    "\n",
    "        # Sum for theta\n",
    "        sum_terms_theta = 2.0 * c_n_pos_coeffs * torch.cos(n_pi_x) * exp_decay\n",
    "        theta_val += torch.sum(sum_terms_theta, dim=1, keepdim=True)\n",
    "\n",
    "        # Sum for theta_x\n",
    "        sum_terms_theta_x = 2.0 * c_n_pos_coeffs * (-n_pos_vals * torch.pi * torch.sin(n_pi_x)) * exp_decay\n",
    "        theta_x_val += torch.sum(sum_terms_theta_x, dim=1, keepdim=True)\n",
    "\n",
    "        epsilon = 1e-12 # Small epsilon to prevent division by zero if theta is numerically zero\n",
    "        u_final = -2.0 * nu * (theta_x_val / (theta_val + epsilon))\n",
    "\n",
    "        return u_final.view_as(x_eval)\n",
    "\n",
    "class KdVPDE(PDEProblem):\n",
    "    def __init__(self, periodic_bc_scheme: str = \"soft_constraint\"):\n",
    "        # kappa_value will be amplitude A\n",
    "        super().__init__(name=\"KdV\",\n",
    "                         input_vars=['t', 'x'],\n",
    "                         output_vars=['u'],\n",
    "                         time_var='t',\n",
    "                         kappa_name=\"A\",\n",
    "                         default_kappa_value=1.0)\n",
    "        self.periodic_bc_scheme = periodic_bc_scheme\n",
    "\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        # Domain needs to be wide enough for solitons\n",
    "        return {'t': (0.0, 4.0 / self.default_kappa_value), # Time to see interaction, scales with 1/A\n",
    "                'x': (-20.0, 20.0)}\n",
    "\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        return {\n",
    "            'u': {\n",
    "                ('t',): 1,  # For u_t\n",
    "                ('x',): 3   # For u_x (in u*u_x) and u_xxx\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_required_derivative_orders_for_bc(self) -> Optional[Dict[str, Dict[Tuple[str, ...], int]]]:\n",
    "        if self.periodic_bc_scheme == \"soft_constraint\":\n",
    "            return {'u': {('x',): 1}} # Need u_x for periodic u_x\n",
    "        return None\n",
    "\n",
    "    def pde_residual(self, inputs: Dict[str, torch.Tensor],\n",
    "                     model_outputs: torch.Tensor,\n",
    "                     derivatives: Dict[str, torch.Tensor],\n",
    "                     kappa_value: float) -> torch.Tensor:\n",
    "        # PDE: u_t + A * u * u_x + u_xxx = 0\n",
    "        # A = kappa_value\n",
    "        # model_outputs is u(t,x) (shape: batch, 1)\n",
    "        # derivatives: 'd1(u)_dt(1)', 'd1(u)_dx(1)', 'd3(u)_dx(3)'\n",
    "        A = kappa_value\n",
    "        u = model_outputs[:, 0:1]\n",
    "\n",
    "        u_t = derivatives['d1(u)_dt(1)']\n",
    "        u_x = derivatives['d1(u)_dx(1)']\n",
    "        u_xxx = derivatives['d3(u)_dx(3)'] # Trainer needs to provide this key\n",
    "\n",
    "        residual = u_t + A * u * u_x + u_xxx\n",
    "        return residual # Shape (batch, 1)\n",
    "\n",
    "    def _sech(self, x_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return 1.0 / torch.cosh(x_tensor)\n",
    "\n",
    "    def initial_conditions(self, inputs_ic: Dict[str, torch.Tensor],\n",
    "                           model_outputs_ic: torch.Tensor,\n",
    "                           model: nn.Module,\n",
    "                           derivatives_ic: Dict[str, torch.Tensor],\n",
    "                           kappa_value: float) -> torch.Tensor:\n",
    "        # IC: 1-soliton u(0, x) = A * sech^2( sqrt(A/2) * (x - x0) )\n",
    "        # A = kappa_value. Let x0 be center of domain, e.g., 0 for x in [-L, L]\n",
    "        x0 = 0.0\n",
    "        A = kappa_value\n",
    "        x_ic = inputs_ic['x']\n",
    "\n",
    "        # Argument of sech^2. Ensure A is positive.\n",
    "        if A <= 0: A = 1e-6 # Avoid sqrt of non-positive, or raise error\n",
    "\n",
    "        sqrt_arg = torch.tensor(A / 2.0, dtype=x_ic.dtype, device=x_ic.device)\n",
    "        arg = torch.sqrt(sqrt_arg) * (x_ic - x0)\n",
    "        u_true_ic = A * (self._sech(arg)**2)\n",
    "\n",
    "        loss_ic = torch.mean((model_outputs_ic - u_true_ic)**2)\n",
    "        return loss_ic\n",
    "\n",
    "    def boundary_conditions(self, inputs_bc: Dict[str, torch.Tensor],\n",
    "                        model_outputs_bc: torch.Tensor, # Output on all points from get_boundary_points_hyperrect\n",
    "                        derivatives_bc: Dict[str, torch.Tensor], # Derivatives on all those points\n",
    "                        model: nn.Module, # Keep for flexibility\n",
    "                        kappa_value: float) -> torch.Tensor:\n",
    "        if self.periodic_bc_scheme == \"soft_constraint\":\n",
    "            t_b = inputs_bc[self.time_var] # (N_total_bc_pts, 1)\n",
    "            x_b = inputs_bc['x']           # (N_total_bc_pts, 1)\n",
    "\n",
    "            x_min_val, x_max_val = self.get_domain_bounds()['x']\n",
    "\n",
    "            # Indices for points at x_min and x_max\n",
    "            # This assumes get_boundary_points_hyperrect samples t values consistently for both faces\n",
    "            # A robust way to ensure pairing:\n",
    "            # Find all unique t values that appear on BOTH x_min and x_max boundaries\n",
    "\n",
    "            # Simplified pairing assuming structure from get_boundary_points_hyperrect:\n",
    "            # (num_points_per_face for x_min, then num_points_per_face for x_max, with matching t's)\n",
    "            # This relies on num_points_per_face being the same for both faces.\n",
    "            # And t values being sampled identically.\n",
    "\n",
    "            num_pts_on_one_face = x_b[x_b == x_min_val].shape[0]\n",
    "\n",
    "            # Assuming t_coords are sampled consistently for x_min and x_max faces\n",
    "            # and are stacked, e.g., [t_face1, t_face2, ...]\n",
    "            # More careful selection/pairing might be needed if sampling strategy is complex.\n",
    "\n",
    "            # A more direct way for soft periodic BC:\n",
    "            # Generate N_periodic_t time points *within this function*.\n",
    "            # This bypasses the need to perfectly align points from get_boundary_points_hyperrect.\n",
    "\n",
    "            domain_b = self.get_domain_bounds()\n",
    "            t_min_b, t_max_b = domain_b[self.time_var]\n",
    "            x_min_b, x_max_b = domain_b['x']\n",
    "\n",
    "            # Use a portion of the provided BC points to define num_periodic_pts\n",
    "            # num_periodic_pts = model_outputs_bc.shape[0] // 2 # Assuming half on x_min, half on x_max\n",
    "            # Or a fixed number, e.g., from config.num_bc_pts_per_face\n",
    "            num_periodic_pts = inputs_bc[self.time_var][inputs_bc['x'] == x_min_b].shape[0]\n",
    "            if num_periodic_pts == 0: return torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "\n",
    "\n",
    "            t_periodic = torch.rand(num_periodic_pts, 1, device=model_outputs_bc.device) * (t_max_b - t_min_b) + t_min_b\n",
    "            t_periodic.requires_grad_(True) # In case t-derivatives were needed at BC\n",
    "\n",
    "            x_at_min_periodic = torch.full_like(t_periodic, x_min_b).requires_grad_(True)\n",
    "            x_at_max_periodic = torch.full_like(t_periodic, x_max_b).requires_grad_(True)\n",
    "\n",
    "            # Prepare inputs for model\n",
    "            inputs_at_xmin_dict = {self.time_var: t_periodic, 'x': x_at_min_periodic}\n",
    "            inputs_at_xmax_dict = {self.time_var: t_periodic, 'x': x_at_max_periodic}\n",
    "\n",
    "            model_input_xmin = torch.cat([inputs_at_xmin_dict[var] for var in self.input_vars], dim=1)\n",
    "            model_input_xmax = torch.cat([inputs_at_xmax_dict[var] for var in self.input_vars], dim=1)\n",
    "\n",
    "            u_at_xmin = model(model_input_xmin)\n",
    "            u_at_xmax = model(model_input_xmax)\n",
    "            loss_val_periodic = torch.mean((u_at_xmin - u_at_xmax)**2)\n",
    "\n",
    "            loss_deriv_periodic = torch.tensor(0.0, device=loss_val_periodic.device)\n",
    "            # For derivative u_x(t, x_min) = u_x(t, x_max)\n",
    "            # We need to compute derivatives w.r.t x at these specific (t_periodic, x_at_min/max_periodic) points\n",
    "\n",
    "            # This uses the Trainer's _compute_derivatives, which needs the Trainer instance.\n",
    "            # This is a bit of a circular dependency if not handled carefully.\n",
    "            # Alternative: Compute grads directly here.\n",
    "\n",
    "            # Compute u_x at x_min for t_periodic\n",
    "            du_dx_at_xmin = torch.autograd.grad(u_at_xmin, x_at_min_periodic, grad_outputs=torch.ones_like(u_at_xmin), create_graph=True)[0]\n",
    "            # Compute u_x at x_max for t_periodic\n",
    "            du_dx_at_xmax = torch.autograd.grad(u_at_xmax, x_at_max_periodic, grad_outputs=torch.ones_like(u_at_xmax), create_graph=True)[0]\n",
    "\n",
    "            if du_dx_at_xmin is not None and du_dx_at_xmax is not None:\n",
    "                loss_deriv_periodic = torch.mean((du_dx_at_xmin - du_dx_at_xmax)**2)\n",
    "            else:\n",
    "                print(\"Warning: Could not compute derivatives for periodic BC enforcement.\")\n",
    "\n",
    "            return loss_val_periodic + loss_deriv_periodic # Add relative weighting if desired\n",
    "\n",
    "        elif self.periodic_bc_scheme == \"hard_transform\":\n",
    "            raise NotImplementedError(\"Hard periodic BC transform not implemented here.\")\n",
    "        return torch.tensor(0.0, device=model_outputs_bc.device) # Default if no scheme matches\n",
    "\n",
    "    def get_ground_truth(self, inputs: Dict[str, torch.Tensor],\n",
    "                         kappa_value: float) -> Optional[torch.Tensor]:\n",
    "        # 1-soliton solution: u(t, x) = A * sech^2( sqrt(A/2) * (x - A*t - x0) )\n",
    "        # A = kappa_value. Let x0 (initial position) = 0.\n",
    "        # Speed c = A.\n",
    "        x0 = 0.0\n",
    "        A = kappa_value\n",
    "        c_soliton = A\n",
    "\n",
    "        t = inputs['t']\n",
    "        x = inputs['x']\n",
    "\n",
    "        if A <= 0: A = 1e-6 # Avoid issues, or problem is ill-defined for this solution\n",
    "\n",
    "        sqrt_arg = torch.tensor(A / 2.0, dtype=x.dtype, device=x.device)\n",
    "        arg = torch.sqrt(sqrt_arg) * (x - c_soliton * t - x0)\n",
    "        u_true = A * (self._sech(arg)**2)\n",
    "        return u_true\n",
    "\n",
    "class SineGordonPDE(PDEProblem):\n",
    "    def __init__(self, stationary_kink=True):\n",
    "        super().__init__(name=\"SineGordon\",\n",
    "                         input_vars=['t', 'x'],\n",
    "                         output_vars=['u'],\n",
    "                         time_var='t',\n",
    "                         kappa_name=\"beta\", # Coefficient of sin(u)\n",
    "                         default_kappa_value=1.0)\n",
    "        self.stationary_kink = stationary_kink # If true, c=0\n",
    "\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        # Domain needs to be wide enough for solitons/kinks\n",
    "        return {'t': (0.0, 10.0), 'x': (-20.0, 20.0)}\n",
    "\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        return {\n",
    "            'u': {\n",
    "                ('t',): 2,  # For u_tt\n",
    "                ('x',): 2   # For u_xx\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_required_derivative_orders_for_ic(self) -> Optional[Dict[str, Dict[Tuple[str, ...], int]]]:\n",
    "    # We need the PINN's d(u)/dt(1) at t=0 to compare against the analytical u_t(0,x).\n",
    "        return {'u': {('t',): 1}}\n",
    "\n",
    "\n",
    "    def pde_residual(self, inputs: Dict[str, torch.Tensor],\n",
    "                       model_outputs: torch.Tensor,\n",
    "                       derivatives: Dict[str, torch.Tensor],\n",
    "                       kappa_value: float) -> torch.Tensor:\n",
    "        # PDE: u_tt - u_xx + beta * sin(u) = 0. beta = kappa_value\n",
    "        u = model_outputs[:, 0:1]\n",
    "        u_tt = derivatives['d2(u)_dt(2)']\n",
    "        u_xx = derivatives['d2(u)_dx(2)']\n",
    "        beta = kappa_value\n",
    "\n",
    "        residual = u_tt - u_xx + beta * torch.sin(u)\n",
    "        return residual\n",
    "\n",
    "    def _kink_solution_val(self, x: torch.Tensor, t: Optional[torch.Tensor], kappa_value: float, c: float = 0.0) -> torch.Tensor:\n",
    "        # General kink: 4 * arctan(exp(gamma * (x - c*t - x0)))\n",
    "        # gamma = sqrt(kappa / (1-c^2))\n",
    "        # For stationary (c=0): 4 * arctan(exp(sqrt(kappa) * (x - x0)))\n",
    "        # Let x0 = 0\n",
    "        x0 = 0.0\n",
    "        beta = kappa_value\n",
    "        if beta <=0: beta = 1e-6 # ensure positive beta\n",
    "\n",
    "        if abs(c) >= 1.0: raise ValueError(\"Kink speed |c| must be < 1 for this solution form.\")\n",
    "\n",
    "        sqrt_arg = torch.tensor(beta / (1.0 - c**2 + 1e-9), dtype=x.dtype, device=x.device) # Ensure same device\n",
    "        gamma = torch.sqrt(sqrt_arg) # Add epsilon for c very close to 1\n",
    "\n",
    "        arg_exp = x - x0\n",
    "        if t is not None:\n",
    "            arg_exp = arg_exp - c * t\n",
    "\n",
    "        return 4 * torch.arctan(torch.exp(gamma * arg_exp))\n",
    "\n",
    "    def _kink_solution_dt_val(self, x: torch.Tensor, t: torch.Tensor, kappa_value: float, c: float) -> torch.Tensor:\n",
    "        # u_t = -c * gamma * 4 * exp(gamma*(x-ct-x0)) / (1 + exp(2*gamma*(x-ct-x0)))\n",
    "        # u_t = -c * gamma * 2 * sech(gamma*(x-ct-x0))\n",
    "        if c == 0.0:\n",
    "            return torch.zeros_like(x)\n",
    "\n",
    "        x0 = 0.0\n",
    "        beta = kappa_value\n",
    "        if beta <=0: beta = 1e-6\n",
    "        if abs(c) >= 1.0: raise ValueError(\"Kink speed |c| must be < 1 for this solution form.\")\n",
    "        sqrt_arg = torch.tensor(beta / ((1.0 - c**2 + 1e-9)), dtype=x.dtype, device=x.device) # Ensure same device\n",
    "        gamma = torch.sqrt(sqrt_arg)\n",
    "\n",
    "        arg_exp = gamma * (x - c * t - x0)\n",
    "        # Using the sech form: 2 / (exp(arg) + exp(-arg)) = 2 * exp(arg) / (exp(2*arg) + 1)\n",
    "        # Simplified: u_t = -c * deriv_of_u_wrt_arg_of_exp\n",
    "        # u = 4 arctan(exp(Y)), du/dY = 4 * exp(Y) / (1+exp(2Y))\n",
    "        # Y = gamma*(x-ct-x0), dY/dt = -c*gamma\n",
    "        # So u_t = (4 * exp(Y) / (1+exp(2Y))) * (-c*gamma)\n",
    "        exp_Y = torch.exp(arg_exp)\n",
    "        u_t_val = (4 * exp_Y / (1 + exp_Y**2)) * (-c * gamma)\n",
    "        return u_t_val\n",
    "\n",
    "\n",
    "    def initial_conditions(self, inputs_ic: Dict[str, torch.Tensor],\n",
    "                               model_outputs_ic: torch.Tensor,\n",
    "                               derivatives_ic: Dict[str, torch.Tensor],\n",
    "                               model: nn.Module,\n",
    "                               kappa_value: float) -> torch.Tensor:\n",
    "        t_ic = inputs_ic['t'] # Should be all zeros\n",
    "        x_ic = inputs_ic['x']\n",
    "\n",
    "        # Kink speed c. If stationary_kink is True, c=0. Otherwise, pick a c, e.g. 0.5\n",
    "        c_kink = 0.0 if self.stationary_kink else 0.5\n",
    "\n",
    "        u_true_at_ic = self._kink_solution_val(x_ic, t_ic, kappa_value, c=c_kink)\n",
    "        loss_u = torch.mean((model_outputs_ic - u_true_at_ic)**2)\n",
    "\n",
    "        # For u_t(0,x)\n",
    "        # The model doesn't directly output u_t. We need to compute it from the model.\n",
    "        # The IC is on u_t, so we need derivatives_ic['d1(u)_dt(1)'] from the PINN.\n",
    "        u_t_true_at_ic = self._kink_solution_dt_val(x_ic, t_ic, kappa_value, c=c_kink)\n",
    "\n",
    "        loss_ut = torch.tensor(0.0, device=loss_u.device)\n",
    "        if 'd1(u)_dt(1)' in derivatives_ic: # if Trainer provided u_t from PINN\n",
    "            pinn_ut_at_ic = derivatives_ic['d1(u)_dt(1)']\n",
    "            loss_ut = torch.mean((pinn_ut_at_ic - u_t_true_at_ic)**2)\n",
    "        elif c_kink != 0.0: # If u_t is non-zero and not provided by trainer, it's an issue\n",
    "             print(\"Warning (SineGordon IC): Non-zero u_t IC but d1(u)_dt(1) not in derivatives_ic.\")\n",
    "\n",
    "\n",
    "        return loss_u + loss_ut # Add weights if needed\n",
    "\n",
    "    def boundary_conditions(self, inputs_bc: Dict[str, torch.Tensor],\n",
    "                                model_outputs_bc: torch.Tensor,\n",
    "                                derivatives_bc: Dict[str, torch.Tensor],\n",
    "                                model: nn.Module,\n",
    "                                kappa_value: float) -> torch.Tensor:\n",
    "        # Fix u to its analytical kink profile values at boundaries x_min, x_max for all t in inputs_bc\n",
    "        t_bc = inputs_bc['t']\n",
    "        x_bc = inputs_bc['x'] # Contains points at x_min and x_max\n",
    "\n",
    "        x_min_val, x_max_val = self.get_domain_bounds()['x']\n",
    "        c_kink = 0.0 if self.stationary_kink else 0.5\n",
    "\n",
    "        u_true_at_bc = self._kink_solution_val(x_bc, t_bc, kappa_value, c=c_kink)\n",
    "\n",
    "        # Assuming model_outputs_bc corresponds one-to-one with inputs_bc\n",
    "        loss_bc = torch.mean((model_outputs_bc - u_true_at_bc)**2)\n",
    "\n",
    "        # Optional: Fix u_x to analytical u_x at boundaries for smoother enforcement\n",
    "        # ux_true_at_bc = ... (compute analytical du/dx)\n",
    "        # pinn_ux_at_bc = derivatives_bc['d1(u)_dx(1)']\n",
    "        # loss_ux_bc = torch.mean((pinn_ux_at_bc - ux_true_at_bc)**2)\n",
    "        # return loss_bc + loss_ux_bc\n",
    "\n",
    "        return loss_bc\n",
    "\n",
    "    def get_ground_truth(self, inputs: Dict[str, torch.Tensor],\n",
    "                           kappa_value: float) -> Optional[torch.Tensor]:\n",
    "        t = inputs['t']\n",
    "        x = inputs['x']\n",
    "        c_kink = 0.0 if self.stationary_kink else 0.5\n",
    "        return self._kink_solution_val(x, t, kappa_value, c=c_kink)\n",
    "\n",
    "class AllenCahnPDE(PDEProblem):\n",
    "    def __init__(self):\n",
    "        # kappa = 1/D, where D is diffusion coeff. Small D -> sharp interface.\n",
    "        # Let's rename kappa_name to \"inv_D\" for clarity. Default D=0.1 -> kappa=10\n",
    "        super().__init__(name=\"AllenCahn\",\n",
    "                         input_vars=['t', 'x'],\n",
    "                         output_vars=['u'],\n",
    "                         time_var='t',\n",
    "                         kappa_name=\"inv_D\",\n",
    "                         default_kappa_value=10.0)\n",
    "\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        return {'t': (0.0, 5.0), 'x': (-5.0, 5.0)} # Adjust as needed\n",
    "\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        return {\n",
    "            'u': {\n",
    "                ('t',): 1,  # For u_t\n",
    "                ('x',): 2   # For u_xx\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def pde_residual(self, inputs: Dict[str, torch.Tensor],\n",
    "                       model_outputs: torch.Tensor,\n",
    "                       derivatives: Dict[str, torch.Tensor],\n",
    "                       kappa_value: float) -> torch.Tensor:\n",
    "        # PDE: u_t = D * u_xx - u*(u^2-1).  D = 1.0 / kappa_value\n",
    "        # Residual: u_t - D * u_xx + u*(u^2-1) = 0\n",
    "        u = model_outputs[:, 0:1]\n",
    "        u_t = derivatives['d1(u)_dt(1)']\n",
    "        u_xx = derivatives['d2(u)_dx(2)']\n",
    "\n",
    "        inv_D = kappa_value\n",
    "        if inv_D <= 1e-6 : inv_D = 1e-6 # Avoid division by zero if D is kappa\n",
    "        D_coeff = 1.0 / inv_D\n",
    "\n",
    "        # Reaction term f(u) = u^3 - u  (or u(u-1)(u+1))\n",
    "        # The PDE is often u_t = D u_xx - f'(u) where F(u) = 1/4 (u^2-1)^2\n",
    "        # so f'(u) = u(u^2-1).\n",
    "        # Residual: u_t - D * u_xx + u*(u^2-1)\n",
    "        reaction_term = u * (u**2 - 1.0)\n",
    "        residual = u_t - D_coeff * u_xx + reaction_term\n",
    "        return residual\n",
    "\n",
    "    def _stationary_front_val(self, x: torch.Tensor, D_coeff: float) -> torch.Tensor:\n",
    "        # u(x) = tanh(x / sqrt(2*D))\n",
    "        if D_coeff <= 1e-9: D_coeff = 1e-9 # Avoid sqrt of zero or negative\n",
    "\n",
    "        # Convert the argument of sqrt to a tensor on the same device as x\n",
    "        sqrt_arg = torch.tensor(2.0 * D_coeff, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        return torch.tanh(x / torch.sqrt(sqrt_arg))\n",
    "\n",
    "    def initial_conditions(self, inputs_ic: Dict[str, torch.Tensor],\n",
    "                               model_outputs_ic: torch.Tensor,\n",
    "                               derivatives_ic: Dict[str, torch.Tensor],\n",
    "                               model: nn.Module,\n",
    "                               kappa_value: float) -> torch.Tensor:\n",
    "        # IC: u(0, x) = tanh(x / sqrt(2D))\n",
    "        x_ic = inputs_ic['x']\n",
    "        inv_D = kappa_value\n",
    "        if inv_D <= 1e-6 : inv_D = 1e-6\n",
    "        D_coeff = 1.0 / inv_D\n",
    "\n",
    "        u_true_at_ic = self._stationary_front_val(x_ic, D_coeff)\n",
    "        loss_ic = torch.mean((model_outputs_ic - u_true_at_ic)**2)\n",
    "        return loss_ic\n",
    "\n",
    "    def boundary_conditions(self, inputs_bc: Dict[str, torch.Tensor],\n",
    "                                model_outputs_bc: torch.Tensor,\n",
    "                                derivatives_bc: Dict[str, torch.Tensor],\n",
    "                                model: nn.Module,\n",
    "                                kappa_value: float) -> torch.Tensor:\n",
    "        # BC: u(t, x_min) = -1, u(t, x_max) = +1 (for the tanh-like profile)\n",
    "        # These are the asymptotic values of the stationary front.\n",
    "        t_bc = inputs_bc['t'] # Not used if BC is time-independent\n",
    "        x_bc = inputs_bc['x'] # Contains points at x_min and x_max\n",
    "\n",
    "        x_min_val, x_max_val = self.get_domain_bounds()['x']\n",
    "        loss_val = torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "\n",
    "        # Points at x_min\n",
    "        mask_xmin = (x_bc == x_min_val)\n",
    "        if torch.any(mask_xmin):\n",
    "            u_pred_xmin = model_outputs_bc[mask_xmin]\n",
    "            u_true_xmin = torch.full_like(u_pred_xmin, -1.0)\n",
    "            loss_val += torch.mean((u_pred_xmin - u_true_xmin)**2)\n",
    "\n",
    "        # Points at x_max\n",
    "        mask_xmax = (x_bc == x_max_val)\n",
    "        if torch.any(mask_xmax):\n",
    "            u_pred_xmax = model_outputs_bc[mask_xmax]\n",
    "            u_true_xmax = torch.full_like(u_pred_xmax, 1.0)\n",
    "            loss_val += torch.mean((u_pred_xmax - u_true_xmax)**2)\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "    def get_ground_truth(self, inputs: Dict[str, torch.Tensor],\n",
    "                           kappa_value: float) -> Optional[torch.Tensor]:\n",
    "        # For Allen-Cahn, the stationary front u(x) = tanh(x / sqrt(2D)) is a solution if c=0.\n",
    "        # If the initial condition is this front, and BCs match, it should remain stationary.\n",
    "        # So, u(t,x) = tanh(x / sqrt(2D)).\n",
    "        x = inputs['x']\n",
    "        # t = inputs['t'] # Not used for stationary solution\n",
    "\n",
    "        inv_D = kappa_value\n",
    "        if inv_D <= 1e-6 : inv_D = 1e-6\n",
    "        D_coeff = 1.0 / inv_D\n",
    "\n",
    "        return self._stationary_front_val(x, D_coeff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91646ea",
   "metadata": {},
   "source": [
    "## Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57777a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Running Experiment: experiment_data_test/Burgers\\kappa_1p0e01\\act_tanh\\N_30\\D_1\\seed_42 ---\n",
      "Config: ExperimentConfig(pde_name='Burgers', kappa_val=10.0, activation_str='tanh', seed=42, depth=1, width=30, optimizer_type='adam', lr=0.001, epochs=1000, log_epochs_list=[0, 500, 1000], num_test_pts_error_grid=501, loss_weight_pde=1.0, loss_weight_bc=1.0, loss_weight_ic=1.0, M_collocation_pts=None, M_collocation_factor=10, num_total_bc_pts=None, num_bc_pts_per_face=50, num_ic_pts=100, collocation_scheme='uniform')\n",
      "Warning: BurgersPDE ground truth for kappa=10.0 not implemented. Returning None.\n",
      "Epoch 0/1000, Loss: 3.672e+00, L2_err_rel: nan, GradNorm: 0.000e+00\n",
      "Warning: BurgersPDE ground truth for kappa=10.0 not implemented. Returning None.\n",
      "Epoch 500/1000, Loss: 4.370e-01, L2_err_rel: nan, GradNorm: 1.160e-01\n",
      "Warning: BurgersPDE ground truth for kappa=10.0 not implemented. Returning None.\n",
      "Epoch 1000/1000, Loss: 4.057e-01, L2_err_rel: nan, GradNorm: 3.035e-01\n",
      "Training finished. Total active time: 3.86s\n",
      "Finished experiment. Final L2_err_rel: nan\n",
      "\n",
      "--- Running Experiment: experiment_data_test/KdV\\kappa_5p0e-01\\act_tanh\\N_30\\D_1\\seed_42 ---\n",
      "Config: ExperimentConfig(pde_name='KdV', kappa_val=0.5, activation_str='tanh', seed=42, depth=1, width=30, optimizer_type='adam', lr=0.001, epochs=1000, log_epochs_list=[0, 500, 1000], num_test_pts_error_grid=501, loss_weight_pde=1.0, loss_weight_bc=1.0, loss_weight_ic=1.0, M_collocation_pts=None, M_collocation_factor=10, num_total_bc_pts=None, num_bc_pts_per_face=50, num_ic_pts=100, collocation_scheme='uniform')\n",
      "Epoch 0/1000, Loss: 3.538e+01, L2_err_rel: 1.907e+01, GradNorm: 0.000e+00\n",
      "Epoch 500/1000, Loss: 2.736e-03, L2_err_rel: 6.346e-01, GradNorm: 7.145e-02\n",
      "Epoch 1000/1000, Loss: 1.154e-03, L2_err_rel: 5.438e-01, GradNorm: 7.602e-02\n",
      "Training finished. Total active time: 3.94s\n",
      "Finished experiment. Final L2_err_rel: 5.438e-01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    pde_instances_map = {\n",
    "        \"TrivialLinear\": TrivialLinearPDE(),\n",
    "        \"Poisson\": PoissonPDE(),\n",
    "        \"Burgers\": BurgersPDE(), # Ground truth and BCs still placeholders\n",
    "        \"KdV\": KdVPDE(),         # BCs still placeholders\n",
    "        \"SineGordon\": SineGordonPDE(stationary_kink=True), # Test stationary first\n",
    "        # \"SineGordonMoving\": SineGordonPDE(stationary_kink=False), # For later\n",
    "        \"AllenCahn\": AllenCahnPDE(),\n",
    "    }\n",
    "\n",
    "    runner = ExperimentRunner(base_results_dir=\"experiment_data_test/\",\n",
    "                              pde_map=pde_instances_map,\n",
    "                              device=DEVICE)\n",
    "\n",
    "    configs_to_run = []\n",
    "\n",
    "    # --- Test Configs ---\n",
    "    # test_pdes_to_run = [\"TrivialLinear\", \"Poisson\", \"Burgers\", \"KdV\", \"SineGordon\", \"AllenCahn\"] # Focus on one new PDE at a time for initial testing\n",
    "    test_pdes_to_run = [\"Burgers\", \"KdV\"]\n",
    "    test_widths = [30]\n",
    "    test_activations = [\"tanh\"]\n",
    "    test_seeds = [42]\n",
    "    test_epochs = 1000 # Short run for testing\n",
    "\n",
    "    # Define kappa values for each PDE specifically for testing\n",
    "    KAPPA_VALS_MAP_TEST = {\n",
    "        \"TrivialLinear\": [1.0], # Kappa not used\n",
    "        \"Poisson\": [1.0],       # Kappa not used\n",
    "        \"Burgers\": [10.0],      # nu = 0.1 (relatively easy)\n",
    "        \"KdV\": [0.5],           # Amplitude A = 0.5\n",
    "        \"SineGordon\": [0.25],   # beta = 0.25 (weaker nonlinearity)\n",
    "        \"AllenCahn\": [2.0]      # inv_D = 2.0  => D = 0.5 (less sharp interface)\n",
    "    }\n",
    "\n",
    "    for pde_name_to_test in test_pdes_to_run:\n",
    "        if pde_name_to_test not in pde_instances_map:\n",
    "            print(f\"Skipping {pde_name_to_test}, not in pde_instances_map for testing.\")\n",
    "            continue\n",
    "\n",
    "        kappas_for_this_pde = KAPPA_VALS_MAP_TEST.get(pde_name_to_test, [pde_instances_map[pde_name_to_test].default_kappa_value])\n",
    "\n",
    "        for kappa_v_test in kappas_for_this_pde:\n",
    "            for width_v_test in test_widths:\n",
    "                for act_v_test in test_activations:\n",
    "                    for seed_v_test in test_seeds:\n",
    "                        # Ensure log_epochs list is sensible for short test_epochs\n",
    "                        log_epochs_test = sorted(list(set([0, test_epochs // 2, test_epochs])))\n",
    "\n",
    "                        configs_to_run.append(ExperimentConfig(\n",
    "                            pde_name=pde_name_to_test,\n",
    "                            kappa_val=kappa_v_test,\n",
    "                            activation_str=act_v_test,\n",
    "                            seed=seed_v_test,\n",
    "                            depth=1, # SLN\n",
    "                            width=width_v_test,\n",
    "                            M_collocation_factor=10, # M_collocation = width * 10\n",
    "                            # M_collocation_pts = 200, # Or fixed number\n",
    "                            num_bc_pts_per_face=50, # Reasonable fixed number for 1D spatial\n",
    "                            num_ic_pts=100,         # Reasonable fixed number for 1D spatial\n",
    "                            collocation_scheme=\"uniform\",\n",
    "                            optimizer_type=\"adam\",\n",
    "                            lr=1e-3,\n",
    "                            epochs=test_epochs,\n",
    "                            log_epochs_list=log_epochs_test,\n",
    "                            num_test_pts_error_grid=501 # Fewer points for faster testing\n",
    "                        ))\n",
    "\n",
    "    for cfg_test in configs_to_run:\n",
    "        runner.run_single_experiment(cfg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01417fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
