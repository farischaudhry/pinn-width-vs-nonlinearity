{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34179b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quasirandom import SobolEngine\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "\n",
    "class PDEProblem(ABC):\n",
    "    def __init__(\n",
    "            self,\n",
    "            name: str,\n",
    "            input_vars: List[str] = ['x'],\n",
    "            output_vars: List[str] = ['u'],\n",
    "            time_var: Optional[str] = None,\n",
    "            kappa_name: str = \"kappa\",\n",
    "            default_kappa_value: float = 1.0\n",
    "        ):\n",
    "        self.name: str = name\n",
    "        self.input_vars: List[str] = sorted(list(set(input_vars)))\n",
    "        self.output_vars: List[str] = output_vars\n",
    "        self.time_var: Optional[str] = time_var\n",
    "\n",
    "        self.output_dim: int = len(self.output_vars)\n",
    "        self.spatial_domain_dim: int = len(self.input_vars) - (1 if time_var else 0)\n",
    "        self.time_dependent: bool = bool(time_var)\n",
    "\n",
    "        self.kappa_name: str = kappa_name\n",
    "        self.default_kappa_value: float = default_kappa_value\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping input variable names to their (min, max) bounds.\n",
    "        Example: {'x': (0.0, 1.0), 't': (0.0, 2.0)}\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def pde_residual(\n",
    "            self,\n",
    "            inputs: Dict[str, torch.Tensor],\n",
    "            model_outputs: torch.Tensor, # Shape: (batch, output_dim)\n",
    "            derivatives: Dict[str, torch.Tensor], # Keys like 'd(u)_dx(1)', 'd2(u)_dx(2)', 'd(v)_dt(1)' etc.\n",
    "            kappa_value: float\n",
    "        ) -> torch.Tensor: # Expected shape: (batch, num_pde_equations)\n",
    "        \"\"\"\n",
    "        Calculates the PDE residual(s).\n",
    "        - model_outputs: Tensor of shape (batch_size, self.output_dim)\n",
    "        - derivatives: Dictionary where keys might be 'd(out_var)_d(in_var)(order)'\n",
    "                       e.g., 'd1u_dx1' for du/dx, 'd2v_dydt1' for d^2v/dydt.\n",
    "        Should return a tensor where each column is the residual of one PDE equation.\n",
    "        For scalar PDEs, this will be (batch_size, 1).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def boundary_conditions(\n",
    "        self,\n",
    "        inputs_bc: Dict[str, torch.Tensor],\n",
    "        model_outputs_bc: torch.Tensor, # Shape: (batch_bc, output_dim)\n",
    "        model: nn.Module,\n",
    "        kappa_value: float\n",
    "        ) -> torch.Tensor: # Scalar loss term\n",
    "        pass\n",
    "\n",
    "    def initial_conditions(\n",
    "            self,\n",
    "            inputs_ic: Dict[str, torch.Tensor],\n",
    "            model_outputs_ic: torch.Tensor, # Shape: (batch_ic, output_dim)\n",
    "            model: nn.Module,\n",
    "            kappa_value: float\n",
    "        ) -> torch.Tensor: # Scalar loss term\n",
    "        if not self.time_dependent:\n",
    "            device = 'cpu'\n",
    "            if model:\n",
    "                try: device = next(model.parameters()).device\n",
    "                except StopIteration: pass\n",
    "            elif isinstance(model_outputs_ic, torch.Tensor):\n",
    "                device = model_outputs_ic.device\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        raise NotImplementedError(\"Initial conditions must be implemented for time-dependent PDEs.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_ground_truth(self,\n",
    "                         inputs: Dict[str, torch.Tensor],\n",
    "                         kappa_value: float) -> Optional[torch.Tensor]: # Shape: (batch, output_dim)\n",
    "        pass\n",
    "\n",
    "    def get_collocation_points(self,\n",
    "                               num_points: int,\n",
    "                               kappa_value: float,\n",
    "                               device: Union[str, torch.device] = 'cpu',\n",
    "                               strategy: str = 'uniform') -> Dict[str, torch.Tensor]:\n",
    "        domain_bounds = self.get_domain_bounds()\n",
    "        inputs = {}\n",
    "\n",
    "        if strategy == 'sobol':\n",
    "            num_input_dims_for_sampling = len(self.input_vars)\n",
    "            if num_input_dims_for_sampling == 0: # Should not happen for collocation\n",
    "                 return {}\n",
    "            sobol = SobolEngine(dimension=num_input_dims_for_sampling, scramble=True)\n",
    "            # Move Sobol samples to target device after generation\n",
    "            samples_0_1 = sobol.draw(num_points).to(device)\n",
    "\n",
    "        for i, var_name in enumerate(self.input_vars):\n",
    "            if var_name not in domain_bounds:\n",
    "                raise ValueError(f\"Domain bounds not defined for variable: {var_name}\")\n",
    "            var_min, var_max = domain_bounds[var_name]\n",
    "\n",
    "            if strategy == 'uniform':\n",
    "                samples_var = torch.rand(num_points, 1, device=device) * (var_max - var_min) + var_min\n",
    "            elif strategy == 'sobol':\n",
    "                samples_var = samples_0_1[:, i:i+1] * (var_max - var_min) + var_min\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Collocation sampling strategy '{strategy}' not implemented for variable '{var_name}'.\")\n",
    "\n",
    "            inputs[var_name] = samples_var.requires_grad_(True)\n",
    "        return inputs\n",
    "\n",
    "    def get_boundary_points_hyperrect(self,\n",
    "                            num_points_per_face: int,\n",
    "                            kappa_value: float,\n",
    "                            device: Union[str, torch.device] = 'cpu',\n",
    "                            strategy: str = 'uniform') -> Dict[str, torch.Tensor]:\n",
    "        domain_bounds = self.get_domain_bounds()\n",
    "        all_bc_inputs = {v: [] for v in self.input_vars}\n",
    "        # Spatial vars are input_vars excluding the time_var\n",
    "        spatial_vars = [v for v in self.input_vars if v != self.time_var]\n",
    "\n",
    "        if not spatial_vars: # No spatial dimensions, so no spatial boundaries\n",
    "            return {v: torch.empty(0,1,device=device).detach() for v in self.input_vars}\n",
    "\n",
    "        samples_bc_other_dims = None\n",
    "        num_dims_to_sample_on_face = len(spatial_vars) - 1 + (1 if self.time_dependent else 0)\n",
    "\n",
    "        if strategy == 'sobol' and num_dims_to_sample_on_face > 0:\n",
    "            sobol_bc = SobolEngine(dimension=num_dims_to_sample_on_face, scramble=True)\n",
    "            samples_bc_other_dims = sobol_bc.draw(num_points_per_face).to(device)\n",
    "        elif strategy != 'uniform' and strategy != 'sobol': # if strategy is not uniform and sobol setup failed or not chosen\n",
    "            raise NotImplementedError(f\"Boundary sampling strategy '{strategy}' not supported.\")\n",
    "\n",
    "\n",
    "        for fixed_var_name in spatial_vars:\n",
    "            other_sampling_vars = [v for v in spatial_vars if v != fixed_var_name]\n",
    "            if self.time_dependent:\n",
    "                other_sampling_vars.append(self.time_var)\n",
    "\n",
    "            for boundary_value in domain_bounds[fixed_var_name]: # For min and max of this fixed_var\n",
    "                current_face_inputs = {}\n",
    "                current_face_inputs[fixed_var_name] = torch.full((num_points_per_face, 1),\n",
    "                                                                boundary_value, dtype=torch.float32, device=device)\n",
    "\n",
    "                sample_idx = 0\n",
    "                for other_var_name in other_sampling_vars:\n",
    "                    ov_min, ov_max = domain_bounds[other_var_name]\n",
    "                    if strategy == 'sobol' and samples_bc_other_dims is not None:\n",
    "                        current_face_inputs[other_var_name] = samples_bc_other_dims[:, sample_idx:sample_idx+1] * (ov_max - ov_min) + ov_min\n",
    "                        sample_idx +=1\n",
    "                    else: # Default to uniform if Sobol not applicable or not chosen\n",
    "                        current_face_inputs[other_var_name] = torch.rand(num_points_per_face, 1, device=device) * (ov_max - ov_min) + ov_min\n",
    "\n",
    "                # Append points for this face to the main list\n",
    "                for var_n in self.input_vars:\n",
    "                    all_bc_inputs[var_n].append(current_face_inputs[var_n])\n",
    "\n",
    "        # Concatenate points from all faces\n",
    "        final_bc_inputs = {}\n",
    "        for var_n in self.input_vars:\n",
    "            if all_bc_inputs[var_n]: # If any points were added for this variable\n",
    "                final_bc_inputs[var_n] = torch.cat(all_bc_inputs[var_n], dim=0).detach() # BC points usually don't need grad\n",
    "            else: # Should only happen if input_vars is empty or logic error\n",
    "                final_bc_inputs[var_n] = torch.empty(0,1,device=device).detach()\n",
    "        return final_bc_inputs\n",
    "\n",
    "    def get_boundary_points_general(self,\n",
    "                                       num_total_points: int, # Note: parameter name change\n",
    "                                       kappa_value: float,\n",
    "                                       device: Union[str, torch.device] = 'cpu',\n",
    "                                       strategy: str = 'uniform' # Strategy for sampling on the general boundary\n",
    "                                      ) -> Optional[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        To be implemented by subclasses for non-rectangular/complex domains.\n",
    "        Should return points lying *on* the boundary.\n",
    "        Returns None to indicate this method is not implemented or not applicable,\n",
    "        allowing fallback to get_boundary_points_hyperrect.\n",
    "        \"\"\"\n",
    "        return None  # Indicating no general boundary points available\n",
    "\n",
    "    def get_initial_points(self,\n",
    "                           num_points: int,\n",
    "                           kappa_value: float,\n",
    "                           device: Union[str, torch.device] = 'cpu',\n",
    "                           strategy: str = 'uniform') -> Dict[str, torch.Tensor]:\n",
    "        if not self.time_dependent:\n",
    "            return {v: torch.empty(0,1,device=device).requires_grad_(False) for v in self.input_vars}\n",
    "\n",
    "        domain_bounds = self.get_domain_bounds()\n",
    "        inputs = {}\n",
    "\n",
    "        t_initial_val = domain_bounds[self.time_var][0]\n",
    "        inputs[self.time_var] = torch.full((num_points, 1), t_initial_val, dtype=torch.float32, device=device)\n",
    "\n",
    "        spatial_vars = [v for v in self.input_vars if v != self.time_var]\n",
    "        if strategy == 'sobol' and spatial_vars: # only use sobol if there are spatial vars to sample\n",
    "            sobol_ic = SobolEngine(dimension=len(spatial_vars), scramble=True)\n",
    "            samples_0_1_ic = sobol_ic.draw(num_points).to(device)\n",
    "        elif strategy != 'uniform' and strategy != 'sobol':\n",
    "             raise NotImplementedError(f\"IC sampling strategy '{strategy}' not supported.\")\n",
    "\n",
    "\n",
    "        for i, var_name in enumerate(spatial_vars):\n",
    "            var_min, var_max = domain_bounds[var_name]\n",
    "            if strategy == 'uniform':\n",
    "                inputs[var_name] = torch.rand(num_points, 1, device=device) * (var_max - var_min) + var_min\n",
    "            elif strategy == 'sobol' and spatial_vars: # check spatial_vars again for safety\n",
    "                inputs[var_name] = samples_0_1_ic[:, i:i+1] * (var_max - var_min) + var_min\n",
    "            # No else needed if strategy check is done above\n",
    "\n",
    "        # Ensure all input_vars keys are present, even if fixed (like time)\n",
    "        for var_name in self.input_vars:\n",
    "            if var_name not in inputs: # e.g. if only time_var and no spatial_vars\n",
    "                 if var_name == self.time_var: continue # already handled\n",
    "                 # This case should be rare if input_vars is setup correctly with domain_bounds\n",
    "                 inputs[var_name] = torch.empty(num_points, 1, device=device) # or handle error\n",
    "\n",
    "            inputs[var_name].requires_grad_(False) # IC coords generally don't need grad\n",
    "        return inputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary specifying derivative requirements for each output variable.\n",
    "        Structure:\n",
    "          {\n",
    "            'output_var_name_1': { # For the first output variable (e.g., 'u')\n",
    "                # Simple derivatives:\n",
    "                ('input_var_for_deriv',): order,  # e.g., ('x',): 2 for d2(u)/dx2\n",
    "                # Mixed derivatives (sequence of differentiation):\n",
    "                ('input_var_1', 'input_var_2', ...): 1, # e.g., ('x', 'y'): 1 for d/dy(d(u)/dx)\n",
    "                                                       # The value (e.g., 1) indicates one application\n",
    "                                                       # of this sequence of differentiations.\n",
    "            },\n",
    "            'output_var_name_2': { ... } # For the second output variable (e.g., 'v')\n",
    "          }\n",
    "        Example for -u_xx - u_yy = f (output_vars=['u']):\n",
    "          {'u': {('x',): 2, ('y',): 2}}\n",
    "        Example for u_t + v_x = 0, v_t + u_x = 0 (output_vars=['u', 'v']):\n",
    "          {\n",
    "            'u': {('t',): 1},\n",
    "            'v': {('x',): 1, ('t',): 1} # Here u_x is not directly a derivative of 'v',\n",
    "                                        # but if 'v' appears in an equation with u_x,\n",
    "                                        # the PDE residual itself handles fetching u_x.\n",
    "                                        # This dict is about derivatives OF the key output_var_name.\n",
    "                                        # Let's refine this point below.\n",
    "          }\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculate_specific_observables(self,\n",
    "                                       inputs: Dict[str, torch.Tensor],\n",
    "                                       model_outputs: torch.Tensor,\n",
    "                                       ground_truth_outputs: Optional[torch.Tensor],\n",
    "                                       kappa_value: float) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculates PDE-specific physical observables and their errors.\n",
    "        To be implemented by subclasses if relevant.\n",
    "        Args:\n",
    "            inputs: Dictionary of input tensors for the test grid.\n",
    "            model_outputs: Tensor of model predictions on the test grid.\n",
    "            ground_truth_outputs: Tensor of ground truth solutions on the test grid (if available).\n",
    "            kappa_value: Current hardness parameter.\n",
    "        Returns:\n",
    "            A dictionary of observable names to their scalar values (e.g., errors).\n",
    "            Example: {'soliton_amplitude_error': 0.01, 'shock_speed_error': 0.05}\n",
    "        \"\"\"\n",
    "        return {} # Default implementation returns no specific observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5dbb8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "\n",
    "def create_pinn_model(\n",
    "    input_dim: int,\n",
    "    output_dim: int,\n",
    "    n_neurons_per_layer: int,\n",
    "    n_hidden_layers: int = 1, # Default to SLN\n",
    "    activation_str: str = \"tanh\",\n",
    "    device: Union[str, torch.device] = 'cpu'\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Creates a feedforward neural network (PINN model).\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input (e.g., 1 for u(x), 2 for u(x,t)).\n",
    "        output_dim (int): Dimension of the output (e.g., 1 for scalar u).\n",
    "        n_neurons_per_layer (int): Number of neurons in each hidden layer.\n",
    "        n_hidden_layers (int): Number of hidden layers. Default is 1.\n",
    "        activation_str (str): Activation function to use ('tanh', 'relu', 'sigmoid', 'leakyrelu').\n",
    "                              Default is 'tanh'.\n",
    "        device (Union[str, torch.device]): Device to send the model to ('cpu' or 'cuda').\n",
    "                                           Default is 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The PyTorch neural network model (nn.Sequential).\n",
    "    \"\"\"\n",
    "    layers: list[nn.Module] = []\n",
    "\n",
    "    if n_hidden_layers == 0: # Special case: linear model (no hidden layers)\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "    else:\n",
    "        layers.append(nn.Linear(input_dim, n_neurons_per_layer))\n",
    "\n",
    "        # Activation function selection\n",
    "        if activation_str.lower() == 'tanh':\n",
    "            activation_fn: nn.Module = nn.Tanh()\n",
    "        elif activation_str.lower() == 'relu':\n",
    "            activation_fn: nn.Module = nn.ReLU()\n",
    "        elif activation_str.lower() == 'sigmoid':\n",
    "            activation_fn: nn.Module = nn.Sigmoid()\n",
    "        elif activation_str.lower() == 'leakyrelu':\n",
    "            activation_fn: nn.Module = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(n_neurons_per_layer, n_neurons_per_layer))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        # Output layer (connects last hidden layer to output_dim)\n",
    "        layers.append(nn.Linear(n_neurons_per_layer, output_dim))\n",
    "\n",
    "    model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "    # Apply initializations\n",
    "    for i, layer in enumerate(model):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            if activation_str.lower() == 'tanh' or activation_str.lower() == 'sigmoid':\n",
    "                nn.init.xavier_normal_(layer.weight) # Glorot normal\n",
    "            elif activation_str.lower() == 'relu' or activation_str.lower() == 'leakyrelu':\n",
    "                # For Kaiming, if the next layer is an activation, use that info.\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu' if activation_str.lower() == 'relu' else 'leaky_relu')\n",
    "\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, pde_problem: 'PDEProblem', optimizer_str=\"adam\", learning_rate=1e-3, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.pde_problem = pde_problem # Type hint for clarity\n",
    "        self.device = device\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        if optimizer_str.lower() == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer_str.lower() == \"lbfgs\":\n",
    "            self.optimizer = optim.LBFGS(self.model.parameters(), lr=self.lr, max_iter=20, line_search_fn=\"strong_wolfe\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_str}\")\n",
    "\n",
    "        self.optimizer_str = optimizer_str\n",
    "        self.epoch_wise_log = []\n",
    "\n",
    "    def _prepare_model_input(self, inputs_dict: dict) -> torch.Tensor | None:\n",
    "        \"\"\"\n",
    "        Prepares a single tensor input for the model from the inputs_dict.\n",
    "        The order of concatenation is defined by self.pde_problem.input_vars.\n",
    "        \"\"\"\n",
    "        if not inputs_dict:\n",
    "            return None # Or handle as appropriate if model expects input even for empty dict\n",
    "\n",
    "        ordered_input_tensors = []\n",
    "        for var_name in self.pde_problem.input_vars:\n",
    "            if var_name in inputs_dict:\n",
    "                ordered_input_tensors.append(inputs_dict[var_name])\n",
    "            else:\n",
    "                # This should ideally not happen if PDEProblem methods are consistent\n",
    "                raise ValueError(f\"Input variable '{var_name}' expected by PDEProblem.input_vars \"\n",
    "                                 f\"but not found in provided inputs_dict keys: {list(inputs_dict.keys())}\")\n",
    "\n",
    "        if not ordered_input_tensors: # Should be caught by the first check if inputs_dict is empty\n",
    "             return torch.empty(0, device=self.device)\n",
    "\n",
    "        return torch.cat(ordered_input_tensors, dim=1)\n",
    "\n",
    "    def _compute_derivatives(self, inputs_dict_with_grad: Dict[str, torch.Tensor],\n",
    "                             model_outputs_tensor: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes derivatives based on pde_problem.get_required_derivative_orders().\n",
    "        model_outputs_tensor has shape (batch, pde_problem.output_dim)\n",
    "        Derivatives are taken with respect to the individual tensors in inputs_dict_with_grad.\n",
    "\n",
    "        Returns a dictionary of derivatives.\n",
    "        Naming convention examples:\n",
    "        - d(u)_dx(1)       for first derivative of 'u' wrt 'x'\n",
    "        - d2(u)_dx(2)      for second derivative of 'u' wrt 'x'\n",
    "        - d(v)_dt(1)       for first derivative of 'v' wrt 't'\n",
    "        - d(u)_dx(1)dy(1)  for d/dy(du/dx)\n",
    "        \"\"\"\n",
    "        derivatives: Dict[str, torch.Tensor] = {}\n",
    "        required_specs = self.pde_problem.get_required_derivative_orders()\n",
    "        output_var_names = self.pde_problem.output_vars # List of names like ['u', 'v']\n",
    "\n",
    "        for out_idx, out_var_name in enumerate(output_var_names):\n",
    "            if out_var_name not in required_specs: # If no derivatives are listed for this output var\n",
    "                continue\n",
    "\n",
    "            # current_output_component is (batch_size, 1)\n",
    "            current_output_component = model_outputs_tensor[:, out_idx:out_idx+1]\n",
    "\n",
    "            spec_for_this_output_var = required_specs[out_var_name]\n",
    "\n",
    "            for input_var_sequence, order_val in spec_for_this_output_var.items():\n",
    "                # input_var_sequence is a tuple, e.g., ('x',) or ('x', 't')\n",
    "                # order_val for simple derivatives is the max order, e.g., 2 for d2u/dx2\n",
    "                # order_val for mixed sequence is typically 1 (one application of the sequence)\n",
    "\n",
    "                if not isinstance(input_var_sequence, tuple) or not input_var_sequence:\n",
    "                    raise ValueError(f\"Invalid input_var_sequence: {input_var_sequence} for {out_var_name}\")\n",
    "\n",
    "                # --- Handle Simple Derivatives (e.g., ('x',): 2 for d2u/dx2) ---\n",
    "                if len(input_var_sequence) == 1:\n",
    "                    input_var_name_for_deriv = input_var_sequence[0]\n",
    "                    max_order = order_val\n",
    "\n",
    "                    if input_var_name_for_deriv not in inputs_dict_with_grad:\n",
    "                        raise RuntimeError(f\"Input variable '{input_var_name_for_deriv}' needed for derivative of '{out_var_name}' \"\n",
    "                                           f\"not found in inputs_dict_with_grad: {list(inputs_dict_with_grad.keys())}\")\n",
    "                    input_tensor_for_grad = inputs_dict_with_grad[input_var_name_for_deriv]\n",
    "                    if not input_tensor_for_grad.requires_grad:\n",
    "                        raise RuntimeError(f\"Input tensor for '{input_var_name_for_deriv}' does not require grad.\")\n",
    "\n",
    "                    temp_deriv_target = current_output_component\n",
    "                    for o in range(1, max_order + 1):\n",
    "                        grads = torch.autograd.grad(\n",
    "                            outputs=temp_deriv_target,\n",
    "                            inputs=input_tensor_for_grad,\n",
    "                            grad_outputs=torch.ones_like(temp_deriv_target),\n",
    "                            create_graph=True,\n",
    "                            retain_graph=True,\n",
    "                            allow_unused=False # Be strict initially\n",
    "                        )[0]\n",
    "                        if grads is None:\n",
    "                            raise RuntimeError(f\"Gradient for d{o}({out_var_name})_d({input_var_name_for_deriv}){o} was None.\")\n",
    "\n",
    "                        deriv_name = f\"d{'' if o == 1 else o}({out_var_name})_d{input_var_name_for_deriv}({o})\"\n",
    "                        derivatives[deriv_name] = grads\n",
    "                        temp_deriv_target = grads\n",
    "\n",
    "                # --- Handle Mixed Derivatives (e.g., ('x', 't'): 1 for d/dt(du/dx)) ---\n",
    "                elif len(input_var_sequence) > 1:\n",
    "                    if order_val != 1:\n",
    "                        # For now, assume mixed derivative specs like ('x','y'):1 mean one application of d/dy(d/dx(...))\n",
    "                        # Higher order_val for mixed could mean repeated application of the sequence, but that's rare.\n",
    "                        print(f\"Warning: Mixed derivative for {out_var_name} wrt {input_var_sequence} has order_val {order_val} != 1. Interpreting as 1 application.\")\n",
    "\n",
    "                    temp_deriv_target = current_output_component\n",
    "\n",
    "                    # Build the name like \"d(u)_dx(1)dy(1)\"\n",
    "                    # The number before (out_var_name) will be len(input_var_sequence)\n",
    "                    name_prefix = f\"d{len(input_var_sequence)}({out_var_name})_d\"\n",
    "                    name_suffix_parts = []\n",
    "\n",
    "                    for i, invar_name in enumerate(input_var_sequence):\n",
    "                        if invar_name not in inputs_dict_with_grad:\n",
    "                            raise RuntimeError(f\"Input variable '{invar_name}' for mixed derivative of '{out_var_name}' \"\n",
    "                                               f\"not in inputs_dict_with_grad.\")\n",
    "                        input_tensor_for_grad = inputs_dict_with_grad[invar_name]\n",
    "                        if not input_tensor_for_grad.requires_grad:\n",
    "                             raise RuntimeError(f\"Input tensor for mixed deriv '{invar_name}' does not require grad.\")\n",
    "\n",
    "                        grads = torch.autograd.grad(\n",
    "                            outputs=temp_deriv_target,\n",
    "                            inputs=input_tensor_for_grad,\n",
    "                            grad_outputs=torch.ones_like(temp_deriv_target),\n",
    "                            create_graph=True, # Must be true if any further grads in sequence\n",
    "                            retain_graph=True, # Must be true\n",
    "                            allow_unused=False\n",
    "                        )[0]\n",
    "                        if grads is None:\n",
    "                            raise RuntimeError(f\"Mixed derivative part d/d{invar_name} for {out_var_name} failed.\")\n",
    "                        temp_deriv_target = grads\n",
    "                        name_suffix_parts.append(f\"{invar_name}(1)\")\n",
    "\n",
    "                    deriv_name = name_prefix + \"\".join(name_suffix_parts)\n",
    "                    derivatives[deriv_name] = temp_deriv_target\n",
    "        return derivatives\n",
    "\n",
    "    def _calculate_error_metrics_on_test_grid(self, kappa_value, num_test_pts=1001):\n",
    "        self.model.eval()\n",
    "        domain_bounds = self.pde_problem.get_domain_bounds()\n",
    "        test_inputs_dict_for_gt = {} # Populate this as before based on input_vars\n",
    "\n",
    "        # ... (grid generation logic as in your full code for 1D/2D inputs) ...\n",
    "        if len(self.pde_problem.input_vars) == 1:\n",
    "            var_name = self.pde_problem.input_vars[0]\n",
    "            var_min, var_max = domain_bounds[var_name]\n",
    "            test_values_np = np.linspace(var_min, var_max, num_test_pts)\n",
    "            test_values_torch = torch.tensor(test_values_np, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "            test_inputs_dict_for_gt[var_name] = test_values_torch\n",
    "        elif len(self.pde_problem.input_vars) == 2:\n",
    "            var1_name, var2_name = self.pde_problem.input_vars[0], self.pde_problem.input_vars[1]\n",
    "            var1_min, var1_max = domain_bounds[var1_name]\n",
    "            var2_min, var2_max = domain_bounds[var2_name]\n",
    "            pts_per_dim = int(np.sqrt(num_test_pts))\n",
    "            # ... (meshgrid logic) ...\n",
    "            # (ensure num_test_pts is updated based on actual grid size)\n",
    "            var1_vals = torch.linspace(var1_min, var1_max, pts_per_dim, device=self.device)\n",
    "            var2_vals = torch.linspace(var2_min, var2_max, pts_per_dim, device=self.device)\n",
    "            grid_var1, grid_var2 = torch.meshgrid(var1_vals, var2_vals, indexing='ij')\n",
    "            test_inputs_dict_for_gt[var1_name] = grid_var1.reshape(-1, 1)\n",
    "            test_inputs_dict_for_gt[var2_name] = grid_var2.reshape(-1, 1)\n",
    "            num_test_pts = test_inputs_dict_for_gt[var1_name].shape[0]\n",
    "        else:\n",
    "            # For >2D, you'll need to implement a more general grid creation or accept it as an argument\n",
    "            # For now, let's assume we won't hit this for the workshop's core PDEs\n",
    "            print(\"Warning: Test grid generation for >2 input_vars not fully implemented in error metrics.\")\n",
    "            # Fallback or raise error\n",
    "            return {key: float('nan') for key in ['L1_err', 'L2_err', 'Linf_err', 'L1_err_rel',\n",
    "                                                  'L2_err_rel', 'Linf_err_rel', 'PDE_residual_max',\n",
    "                                                  'error_median_abs', 'error_p90_abs']}\n",
    "\n",
    "\n",
    "        test_model_input_tensor = self._prepare_model_input(test_inputs_dict_for_gt)\n",
    "        if test_model_input_tensor is None or test_model_input_tensor.numel() == 0:\n",
    "             print(\"Warning: No test model input tensor generated for error metrics.\")\n",
    "             return {key: float('nan') for key in ['L1_err', 'L2_err', 'Linf_err', 'L1_err_rel',\n",
    "                                                  'L2_err_rel', 'Linf_err_rel', 'PDE_residual_max',\n",
    "                                                  'error_median_abs', 'error_p90_abs']}\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            u_pred_test = self.model(test_model_input_tensor)\n",
    "\n",
    "        u_true_test_torch = self.pde_problem.get_ground_truth(test_inputs_dict_for_gt, kappa_value)\n",
    "\n",
    "        if u_true_test_torch is not None and u_pred_test.shape != u_true_test_torch.shape:\n",
    "            try: u_pred_test = u_pred_test.reshape_as(u_true_test_torch)\n",
    "            except RuntimeError: print(f\"Warning: Cannot reshape u_pred_test for error calc.\")\n",
    "\n",
    "        metrics = {\n",
    "            'L1_err': float('nan'), 'L2_err': float('nan'), 'Linf_err': float('nan'),\n",
    "            'L1_err_rel': float('nan'), 'L2_err_rel': float('nan'), 'Linf_err_rel': float('nan'),\n",
    "            'PDE_residual_max': float('nan'),\n",
    "            'error_median_abs': float('nan'), 'error_p90_abs': float('nan') # New\n",
    "        }\n",
    "\n",
    "        if u_true_test_torch is not None:\n",
    "            error_vec = (u_pred_test - u_true_test_torch).flatten() # Flatten for norms and quantiles\n",
    "            actual_num_test_pts = len(error_vec)\n",
    "            if actual_num_test_pts == 0: actual_num_test_pts = 1\n",
    "\n",
    "            metrics['L1_err'] = torch.linalg.norm(error_vec, ord=1).item() / actual_num_test_pts\n",
    "            metrics['L2_err'] = torch.linalg.norm(error_vec, ord=2).item() / np.sqrt(actual_num_test_pts)\n",
    "            metrics['Linf_err'] = torch.linalg.norm(error_vec, ord=float('inf')).item()\n",
    "\n",
    "            abs_error_vec = torch.abs(error_vec)\n",
    "            metrics['error_median_abs'] = torch.median(abs_error_vec).item()\n",
    "            if actual_num_test_pts > 0 : # Quantile needs at least one element\n",
    "                 metrics['error_p90_abs'] = torch.quantile(abs_error_vec, 0.9).item()\n",
    "\n",
    "            u_true_flat = u_true_test_torch.flatten()\n",
    "            norm_u_true_l1 = torch.linalg.norm(u_true_flat, ord=1)\n",
    "            norm_u_true_l2 = torch.linalg.norm(u_true_flat, ord=2)\n",
    "            norm_u_true_linf = torch.linalg.norm(u_true_flat, ord=float('inf'))\n",
    "\n",
    "            if norm_u_true_l1 > 1e-9: metrics['L1_err_rel'] = torch.linalg.norm(error_vec, ord=1).item() / norm_u_true_l1.item()\n",
    "            if norm_u_true_l2 > 1e-9: metrics['L2_err_rel'] = torch.linalg.norm(error_vec, ord=2).item() / norm_u_true_l2.item()\n",
    "            if norm_u_true_linf > 1e-9: metrics['Linf_err_rel'] = metrics['Linf_err'] / norm_u_true_linf.item()\n",
    "\n",
    "        # Max PDE residual\n",
    "        test_inputs_dict_for_res = {}\n",
    "        for k, v_test in test_inputs_dict_for_gt.items():\n",
    "            if v_test.numel() > 0: # Only process if tensor is not empty\n",
    "                test_inputs_dict_for_res[k] = v_test.clone().detach().requires_grad_(True)\n",
    "\n",
    "        if test_inputs_dict_for_res: # Proceed only if there are inputs for residual calculation\n",
    "            res_model_input_tensor = self._prepare_model_input(test_inputs_dict_for_res)\n",
    "            if res_model_input_tensor is not None and res_model_input_tensor.numel() > 0:\n",
    "                u_pred_for_res = self.model(res_model_input_tensor)\n",
    "                derivatives_for_res = self._compute_derivatives(test_inputs_dict_for_res, u_pred_for_res)\n",
    "                pde_res_vals_on_grid = self.pde_problem.pde_residual(test_inputs_dict_for_res, u_pred_for_res, derivatives_for_res, kappa_value)\n",
    "                if pde_res_vals_on_grid is not None and pde_res_vals_on_grid.numel() > 0 :\n",
    "                    metrics['PDE_residual_max'] = torch.max(torch.abs(pde_res_vals_on_grid.detach())).item()\n",
    "\n",
    "        # Calculate specific observables if the PDEProblem has this method\n",
    "        if hasattr(self.pde_problem, 'calculate_specific_observables'):\n",
    "            try:\n",
    "                specific_obs = self.pde_problem.calculate_specific_observables(\n",
    "                    test_inputs_dict_for_gt, # The dict of input tensors for the test grid\n",
    "                    u_pred_test,             # Model predictions on the test grid\n",
    "                    u_true_test_torch,       # Ground truth on the test grid\n",
    "                    kappa_value\n",
    "                )\n",
    "                if specific_obs and isinstance(specific_obs, dict):\n",
    "                    metrics.update(specific_obs) # Add them to the metrics dict for this epoch\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error calculating specific observables for {self.pde_problem.name}: {e}\")\n",
    "\n",
    "        self.model.train()\n",
    "        return metrics\n",
    "\n",
    "    def _closure_lbfgs(self, collocation_points_dict, bc_points_dict, ic_points_dict, kappa_value, loss_weights):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # PDE Loss\n",
    "        colloc_model_input = self._prepare_model_input(collocation_points_dict)\n",
    "        model_outputs_colloc = self.model(colloc_model_input)\n",
    "        derivatives_colloc = self._compute_derivatives(collocation_points_dict, model_outputs_colloc)\n",
    "        pde_res = self.pde_problem.pde_residual(collocation_points_dict, model_outputs_colloc, derivatives_colloc, kappa_value)\n",
    "        loss_pde = torch.mean(pde_res**2)\n",
    "\n",
    "        # BC Loss\n",
    "        loss_bc = torch.tensor(0.0, device=self.device)\n",
    "        if bc_points_dict: # Check if not empty\n",
    "            bc_model_input = self._prepare_model_input(bc_points_dict)\n",
    "            model_outputs_bc = self.model(bc_model_input)\n",
    "            loss_bc = self.pde_problem.boundary_conditions(bc_points_dict, model_outputs_bc, self.model, kappa_value)\n",
    "\n",
    "        # IC Loss\n",
    "        loss_ic = torch.tensor(0.0, device=self.device)\n",
    "        if self.pde_problem.time_dependent and ic_points_dict:\n",
    "            ic_model_input = self._prepare_model_input(ic_points_dict)\n",
    "            model_outputs_ic = self.model(ic_model_input)\n",
    "            loss_ic = self.pde_problem.initial_conditions(ic_points_dict, model_outputs_ic, self.model, kappa_value)\n",
    "\n",
    "        total_loss = (loss_weights['pde'] * loss_pde +\n",
    "                      loss_weights['bc'] * loss_bc +\n",
    "                      loss_weights['ic'] * loss_ic)\n",
    "        total_loss.backward()\n",
    "        self._current_losses = {'pde': loss_pde.item(), 'bc': loss_bc.item(),\n",
    "                                'ic': loss_ic.item(), 'total': total_loss.item()}\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, num_epochs, kappa_value,\n",
    "              num_collocation_pts, num_bc_pts_per_face, num_ic_pts, # Renamed for clarity\n",
    "              collocation_strategy='uniform',\n",
    "              loss_weights={'pde': 1.0, 'bc': 1.0, 'ic': 1.0},\n",
    "              log_epochs=[0, 1000, 5000, 10000],\n",
    "              num_test_pts_error_grid=1001):\n",
    "\n",
    "        cumulative_time_s = 0.0\n",
    "        self.epoch_wise_log = []\n",
    "\n",
    "        for epoch in range(num_epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Common point sampling (moved outside optimizer-specific block)\n",
    "            # These return dicts like {'x': tensor, 't': tensor}\n",
    "            collocation_points_dict = self.pde_problem.get_collocation_points(\n",
    "                num_collocation_pts, kappa_value, self.device, collocation_strategy\n",
    "            )\n",
    "\n",
    "            bc_points_dict = self.pde_problem.get_boundary_points_general(\n",
    "                num_bc_pts_per_face, kappa_value, self.device, strategy=collocation_strategy # Use same strategy for BCs\n",
    "            )\n",
    "            if bc_points_dict is None: # Fallback to hyperrect if general not implemented\n",
    "                bc_points_dict = self.pde_problem.get_boundary_points_hyperrect(\n",
    "                    num_bc_pts_per_face, kappa_value, self.device, strategy=collocation_strategy\n",
    "                )\n",
    "\n",
    "            ic_points_dict = {}\n",
    "            if self.pde_problem.time_dependent:\n",
    "                ic_points_dict = self.pde_problem.get_initial_points(\n",
    "                    num_ic_pts, kappa_value, self.device, strategy=collocation_strategy # Use same strategy for ICs\n",
    "                )\n",
    "\n",
    "            if self.optimizer_str == \"adam\":\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # PDE Loss\n",
    "                colloc_model_input = self._prepare_model_input(collocation_points_dict)\n",
    "                model_outputs_colloc = self.model(colloc_model_input)\n",
    "                # Pass original dict with grad-enabled tensors for derivative computation\n",
    "                derivatives_colloc = self._compute_derivatives(collocation_points_dict, model_outputs_colloc)\n",
    "                pde_res = self.pde_problem.pde_residual(collocation_points_dict, model_outputs_colloc, derivatives_colloc, kappa_value)\n",
    "                loss_pde = torch.mean(pde_res**2)\n",
    "\n",
    "                # BC Loss\n",
    "                loss_bc = torch.tensor(0.0, device=self.device)\n",
    "                if bc_points_dict:\n",
    "                    bc_model_input = self._prepare_model_input(bc_points_dict)\n",
    "                    model_outputs_bc = self.model(bc_model_input)\n",
    "                    loss_bc = self.pde_problem.boundary_conditions(bc_points_dict, model_outputs_bc, self.model, kappa_value)\n",
    "\n",
    "                # IC Loss\n",
    "                loss_ic = torch.tensor(0.0, device=self.device)\n",
    "                if self.pde_problem.time_dependent and ic_points_dict:\n",
    "                    ic_model_input = self._prepare_model_input(ic_points_dict)\n",
    "                    model_outputs_ic = self.model(ic_model_input)\n",
    "                    loss_ic = self.pde_problem.initial_conditions(ic_points_dict, model_outputs_ic, self.model, kappa_value)\n",
    "\n",
    "                total_loss = (loss_weights['pde'] * loss_pde +\n",
    "                              loss_weights['bc'] * loss_bc +\n",
    "                              loss_weights['ic'] * loss_ic)\n",
    "\n",
    "                if epoch > 0:\n",
    "                    total_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                current_total_loss = total_loss.item()\n",
    "                current_pde_loss = loss_pde.item()\n",
    "                current_bc_loss = loss_bc.item()\n",
    "                current_ic_loss = loss_ic.item()\n",
    "\n",
    "            elif self.optimizer_str == \"lbfgs\":\n",
    "                if epoch > 0:\n",
    "                    self.model.train()\n",
    "                    self.optimizer.step(lambda: self._closure_lbfgs(\n",
    "                        collocation_points_dict, bc_points_dict, ic_points_dict, kappa_value, loss_weights\n",
    "                    ))\n",
    "                # For LBFGS, losses are updated within the closure\n",
    "                current_total_loss = self._current_losses.get('total', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "                current_pde_loss = self._current_losses.get('pde', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "                current_bc_loss = self._current_losses.get('bc', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "                current_ic_loss = self._current_losses.get('ic', float('nan')) if hasattr(self, '_current_losses') else float('nan')\n",
    "\n",
    "            # ... (rest of logging logic is good) ...\n",
    "            epoch_duration_s = time.time() - epoch_start_time\n",
    "            if epoch > 0 : cumulative_time_s += epoch_duration_s\n",
    "\n",
    "            if epoch in log_epochs or epoch == num_epochs:\n",
    "                grad_norm = 0.0\n",
    "                if epoch > 0:\n",
    "                    for p in self.model.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            grad_norm += p.grad.detach().data.norm(2).item() ** 2\n",
    "                    grad_norm = grad_norm ** 0.5 if grad_norm > 0 else 0.0\n",
    "\n",
    "                error_metrics_on_grid = self._calculate_error_metrics_on_test_grid(kappa_value, num_test_pts_error_grid)\n",
    "\n",
    "                # Track L2 norm of weights for regularization and diagnostics\n",
    "                l2_norm_weights = 0.0\n",
    "                for param in self.model.parameters():\n",
    "                    if param.requires_grad: # Usually all model parameters do\n",
    "                        l2_norm_weights += torch.linalg.norm(param.data).item()**2\n",
    "                l2_norm_weights = np.sqrt(l2_norm_weights) if l2_norm_weights > 0 else 0.0\n",
    "\n",
    "                gpu_mem_peak_mb = float('nan')\n",
    "                if self.device.type == 'cuda':\n",
    "                    # Peak memory allocated on this device since the last reset\n",
    "                    gpu_mem_peak_mb = torch.cuda.max_memory_allocated(self.device) / (1024**2) # Convert to MB\n",
    "                    torch.cuda.reset_peak_memory_stats(self.device) # Reset for the next interval\n",
    "\n",
    "                log_entry = {\n",
    "                    'epoch': epoch, 'time_s': cumulative_time_s,\n",
    "                    'loss_total': current_total_loss, 'loss_pde': current_pde_loss,\n",
    "                    'loss_bc': current_bc_loss, 'loss_ic': current_ic_loss,\n",
    "                    'grad_norm_l2': grad_norm, 'l2_norm_weights': l2_norm_weights,\n",
    "                    'gpu_mem_peak_mb': gpu_mem_peak_mb,\n",
    "                }\n",
    "                log_entry.update(error_metrics_on_grid)\n",
    "                self.epoch_wise_log.append(log_entry)\n",
    "\n",
    "                print(f\"Epoch {epoch}/{num_epochs}, Loss: {current_total_loss:.3e}, \"\n",
    "                      f\"L2_err_rel: {log_entry.get('L2_err_rel', float('nan')):.3e}, GradNorm: {grad_norm:.3e}\")\n",
    "\n",
    "        print(f\"Training finished. Total active time: {cumulative_time_s:.2f}s\")\n",
    "        return self.epoch_wise_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dcbc55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict, field\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # Identification\n",
    "    pde_name: str\n",
    "    kappa_val: float\n",
    "    activation_str: str\n",
    "    seed: int\n",
    "\n",
    "    # Architecture\n",
    "    depth: int # Number of hidden layers\n",
    "    width: int # Neurons per hidden layer\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_type: str\n",
    "    lr: float\n",
    "\n",
    "    # Training\n",
    "    epochs: int\n",
    "\n",
    "    # Logging & Error Evaluation\n",
    "    log_epochs_list: list = field(default_factory=lambda: [x for x in range(0, 10001, 100)])\n",
    "    num_test_pts_error_grid: int = 1001\n",
    "\n",
    "    # Loss Weights\n",
    "    loss_weight_pde: float = 1.0\n",
    "    loss_weight_bc: float = 1.0\n",
    "    loss_weight_ic: float = 1.0\n",
    "\n",
    "    # Collocation (points takes precedence over factor)\n",
    "    M_collocation_pts: int = field(default=None)\n",
    "    M_collocation_factor: int = field(default=10)\n",
    "\n",
    "    # IC/BC Points\n",
    "    num_total_bc_pts: Optional[int] = field(default=None) # Total BC points across all faces\n",
    "    num_bc_pts_per_face: Optional[int] = field(default=None) # If not specified, heuristic will be used\n",
    "    num_ic_pts: Optional[int] = field(default=None) # Only for time-dependent PDEs\n",
    "    collocation_scheme: str = field(default='uniform')\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, base_results_dir=\"data/\", pde_map=None, device='cpu'):\n",
    "        self.base_results_dir = base_results_dir\n",
    "        self.pde_map = pde_map if pde_map is not None else {}\n",
    "        self.device = device\n",
    "        os.makedirs(self.base_results_dir, exist_ok=True)\n",
    "\n",
    "    def _get_run_dir(self, config: ExperimentConfig):\n",
    "        # Format kappa_val for filename safety, e.g., replace decimal point\n",
    "        kappa_str = f\"{config.kappa_val:.1e}\".replace('.', 'p').replace('+', '') # e.g. 1p0e-03\n",
    "\n",
    "        run_path = os.path.join(\n",
    "            self.base_results_dir,\n",
    "            config.pde_name,\n",
    "            f\"kappa_{kappa_str}\",\n",
    "            f\"act_{config.activation_str}\",\n",
    "            f\"N_{config.width}\",\n",
    "            f\"D_{config.depth}\",\n",
    "            f\"seed_{config.seed}\"\n",
    "        )\n",
    "        os.makedirs(run_path, exist_ok=True)\n",
    "        return run_path\n",
    "\n",
    "    def run_single_experiment(self, config: ExperimentConfig):\n",
    "        pde_instance = self.pde_map.get(config.pde_name)\n",
    "        if pde_instance is None:\n",
    "            print(f\"Error: PDE problem '{config.pde_name}' not found in pde_map.\")\n",
    "            return\n",
    "\n",
    "        run_dir = self._get_run_dir(config)\n",
    "        print(f\"\\n--- Running Experiment: {run_dir} ---\")\n",
    "        print(f\"Config: {config}\")\n",
    "\n",
    "        with open(os.path.join(run_dir, \"config.json\"), 'w') as f:\n",
    "            json.dump(asdict(config), f, indent=2)\n",
    "\n",
    "        torch.manual_seed(config.seed)\n",
    "        np.random.seed(config.seed)\n",
    "\n",
    "        # Determine model input_dim based on pde_instance.input_vars\n",
    "        # Could also use pde_instance.spatial_domain_dim + (1 if pde_instance.time_dependent else 0)\n",
    "        model_input_dim = len(pde_instance.input_vars)\n",
    "\n",
    "        model = create_pinn_model(\n",
    "            input_dim=model_input_dim,\n",
    "            output_dim=pde_instance.output_dim,\n",
    "            n_hidden_layers=config.depth,\n",
    "            n_neurons_per_layer=config.width,\n",
    "            activation_str=config.activation_str,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(model, pde_instance,\n",
    "                          optimizer_str=config.optimizer_type,\n",
    "                          learning_rate=config.lr,\n",
    "                          device=self.device)\n",
    "\n",
    "\n",
    "        # Collocation points:\n",
    "        if config.M_collocation_pts is not None:\n",
    "            M_collocation = config.M_collocation_pts\n",
    "        else:\n",
    "            # Specified or default collocation factor if direct points not given\n",
    "            M_collocation = config.width * config.M_collocation_factor\n",
    "\n",
    "        # For BC points:\n",
    "        actual_num_bc_pts_per_face = 0\n",
    "        if config.num_total_bc_pts is not None:\n",
    "            num_spatial_dims = pde_instance.spatial_domain_dim\n",
    "            num_faces = 2 * num_spatial_dims if num_spatial_dims > 0 else 0\n",
    "            if num_faces > 0:\n",
    "                actual_num_bc_pts_per_face = config.num_total_bc_pts // num_faces\n",
    "            else: # If no spatial dims, num_total_bc_pts should ideally be 0 or ignored\n",
    "                actual_num_bc_pts_per_face = 0 # or handle appropriately\n",
    "        elif config.num_bc_pts_per_face is not None:\n",
    "            actual_num_bc_pts_per_face = config.num_bc_pts_per_face\n",
    "        else: # Fallback or default heuristic if not specified\n",
    "            num_spatial_dims = pde_instance.spatial_domain_dim\n",
    "            num_faces = 2 * num_spatial_dims if num_spatial_dims > 0 else 0\n",
    "            if num_faces > 0:\n",
    "                heuristic_bc_factor = 20\n",
    "                actual_num_bc_pts_per_face = M_collocation // (heuristic_bc_factor * num_faces)\n",
    "                actual_num_bc_pts_per_face = max(10, actual_num_bc_pts_per_face) # Min points\n",
    "            else:\n",
    "                actual_num_bc_pts_per_face = 0\n",
    "\n",
    "        # For IC points:\n",
    "        actual_num_ic_pts = 0\n",
    "        if pde_instance.time_dependent:\n",
    "            if config.num_ic_pts is not None:\n",
    "                actual_num_ic_pts = config.num_ic_pts\n",
    "            else: # Fallback or default heuristic\n",
    "                heuristic_ic_factor = 10\n",
    "                actual_num_ic_pts = M_collocation // heuristic_ic_factor\n",
    "                actual_num_ic_pts = max(10, actual_num_ic_pts) # Min points\n",
    "\n",
    "        # Adjust log_epochs based on actual config.epochs\n",
    "        actual_log_epochs = [e for e in config.log_epochs_list if e <= config.epochs]\n",
    "        if config.epochs not in actual_log_epochs:\n",
    "            actual_log_epochs.append(config.epochs)\n",
    "        actual_log_epochs = sorted(list(set(actual_log_epochs)))\n",
    "        if 0 not in actual_log_epochs : actual_log_epochs.insert(0,0)\n",
    "\n",
    "\n",
    "        epoch_wise_log_data = trainer.train(\n",
    "            num_epochs=config.epochs,\n",
    "            kappa_value=config.kappa_val,\n",
    "            num_collocation_pts=M_collocation,\n",
    "            num_bc_pts_per_face=actual_num_bc_pts_per_face,\n",
    "            num_ic_pts=actual_num_ic_pts,\n",
    "            collocation_strategy=config.collocation_scheme,\n",
    "            log_epochs=actual_log_epochs,\n",
    "            num_test_pts_error_grid=config.num_test_pts_error_grid\n",
    "        )\n",
    "\n",
    "        df_epoch_log = pd.DataFrame(epoch_wise_log_data)\n",
    "        df_epoch_log.to_csv(os.path.join(run_dir, \"training_log.csv\"), index=False)\n",
    "\n",
    "        final_metrics = {}\n",
    "        if not df_epoch_log.empty:\n",
    "            last_epoch_data = df_epoch_log.iloc[-1]\n",
    "            final_metrics = {\n",
    "                key: last_epoch_data.get(key, float('nan'))\n",
    "                for key in ['time_s', 'loss_total', 'L1_err_rel', 'L2_err_rel',\n",
    "                            'Linf_err_rel', 'PDE_residual_max', 'grad_norm_l2']\n",
    "            }\n",
    "\n",
    "        summary_data = {\"final_metrics\": final_metrics, \"fit_results\": {}} # Config saved separately\n",
    "        with open(os.path.join(run_dir, \"summary.json\"), 'w') as f:\n",
    "            json.dump(summary_data, f, indent=2, cls=NpEncoder) # Handle numpy types if any\n",
    "\n",
    "        print(f\"Finished experiment. Final L2_err_rel: {final_metrics.get('L2_err_rel', 'N/A'):.3e}\")\n",
    "\n",
    "# Helper for JSON serialization if numpy types are used in summary\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Running Experiment: experiment_data_final/Poisson\\kappa_1p0e00\\act_tanh\\N_20\\D_1\\seed_1 ---\n",
      "Config: ExperimentConfig(pde_name='Poisson', kappa_val=1.0, activation_str='tanh', seed=1, depth=1, width=20, optimizer_type='adam', lr=0.001, epochs=2000, log_epochs_list=[0, 1000, 2000], num_test_pts_error_grid=1001, loss_weight_pde=1.0, loss_weight_bc=1.0, loss_weight_ic=1.0, M_collocation_pts=None, M_collocation_factor=10, num_total_bc_pts=None, num_bc_pts_per_face=None, num_ic_pts=None, collocation_scheme='uniform')\n",
      "Epoch 0/2000, Loss: 4.951e-01, L2_err_rel: 9.635e-01, GradNorm: 0.000e+00\n",
      "Epoch 1000/2000, Loss: 3.295e-02, L2_err_rel: 7.841e-02, GradNorm: 9.651e-02\n",
      "Epoch 2000/2000, Loss: 1.830e-04, L2_err_rel: 2.337e-03, GradNorm: 6.572e-03\n",
      "Training finished. Total active time: 3.33s\n",
      "Finished experiment. Final L2_err_rel: 2.337e-03\n",
      "\n",
      "--- Running Experiment: experiment_data_final/Poisson\\kappa_1p0e00\\act_tanh\\N_50\\D_1\\seed_1 ---\n",
      "Config: ExperimentConfig(pde_name='Poisson', kappa_val=1.0, activation_str='tanh', seed=1, depth=1, width=50, optimizer_type='adam', lr=0.001, epochs=2000, log_epochs_list=[0, 1000, 2000], num_test_pts_error_grid=1001, loss_weight_pde=1.0, loss_weight_bc=1.0, loss_weight_ic=1.0, M_collocation_pts=None, M_collocation_factor=10, num_total_bc_pts=None, num_bc_pts_per_face=None, num_ic_pts=None, collocation_scheme='uniform')\n",
      "Epoch 0/2000, Loss: 6.020e-01, L2_err_rel: 3.187e+00, GradNorm: 0.000e+00\n",
      "Epoch 1000/2000, Loss: 4.047e-03, L2_err_rel: 3.611e-02, GradNorm: 3.900e-02\n",
      "Epoch 2000/2000, Loss: 1.951e-04, L2_err_rel: 5.056e-03, GradNorm: 7.084e-03\n",
      "Training finished. Total active time: 3.75s\n",
      "Finished experiment. Final L2_err_rel: 5.056e-03\n",
      "\n",
      "--- Example: How to load and quickly check a result ---\n",
      "Log for first experiment (Poisson, N=20):\n",
      "   epoch  loss_total  L2_err_rel\n",
      "0      0    0.495081    0.963514\n",
      "1   1000    0.032953    0.078406\n",
      "2   2000    0.000183    0.002337\n",
      "\n",
      "Summary of final metrics:\n",
      "  time_s: 3.335e+00\n",
      "  loss_total: 1.830e-04\n",
      "  L1_err_rel: 2.093e-03\n",
      "  L2_err_rel: 2.337e-03\n",
      "  Linf_err_rel: 3.923e-03\n",
      "  PDE_residual_max: 5.407e-02\n",
      "  grad_norm_l2: 6.572e-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution Script (Example) ---\n",
    "if __name__ == '__main__':\n",
    "    # Ensure Cell 1 (PDEProblem) and Cell 2 (create_pinn_model) are executable or imported\n",
    "    # For example, if they are in separate files:\n",
    "    # from pde_problem_cell1 import PDEProblem # (and any concrete PDE classes)\n",
    "    # from model_creator_cell2 import create_pinn_model\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Define Concrete PDEProblem Subclasses Here ---\n",
    "    class TrivialLinearPDE(PDEProblem):\n",
    "        def __init__(self):\n",
    "            super().__init__(name=\"TrivialLinear\", input_vars=['x'], output_vars=['u'], kappa_name=\"kappa\", default_kappa_value=1.0)\n",
    "        def get_domain_bounds(self): return {'x': (0.0, 1.0)}\n",
    "        def pde_residual(self,inputs,model_outputs,derivatives,kappa_value): return derivatives['d2(u)_dx(2)']\n",
    "        def boundary_conditions(self,inputs_bc,model_outputs_bc,model,kappa_value):\n",
    "            x_vals = inputs_bc['x'].squeeze()\n",
    "            loss = torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "            u_at_0 = model_outputs_bc[x_vals == 0.0] - 0.0\n",
    "            u_at_1 = model_outputs_bc[x_vals == 1.0] - 1.0\n",
    "            if u_at_0.numel() > 0: loss += torch.mean(u_at_0**2)\n",
    "            if u_at_1.numel() > 0: loss += torch.mean(u_at_1**2)\n",
    "            return loss\n",
    "        def get_ground_truth(self,inputs,kappa_value):\n",
    "            x = inputs['x']\n",
    "            return x.clone() # u(x) = x\n",
    "        def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "            return {\n",
    "                'u': {       # For output variable 'u'\n",
    "                    ('x',): 2  # We need up to d2u/dx2\n",
    "                }\n",
    "        }\n",
    "\n",
    "\n",
    "    class PoissonPDE(PDEProblem):\n",
    "        def __init__(self):\n",
    "            super().__init__(name=\"Poisson\", input_vars=['x'])\n",
    "            self.forcing_fn = lambda x: torch.sin(np.pi * x)\n",
    "            self.analytical_sol_np = lambda x_np: (1.0 / (np.pi**2)) * np.sin(np.pi * x_np.squeeze())\n",
    "        def get_domain_bounds(self): return {'x': (0.0, 1.0)}\n",
    "        def pde_residual(self,inputs,model_outputs,derivatives,kappa_value):\n",
    "            return derivatives['d2(u)_dx(2)'] + self.forcing_fn(inputs['x'])\n",
    "        def boundary_conditions(self,inputs_bc,model_outputs_bc,model,kappa_value):\n",
    "            x_vals = inputs_bc['x'].squeeze()\n",
    "            loss = torch.tensor(0.0, device=model_outputs_bc.device)\n",
    "            u_at_0 = model_outputs_bc[x_vals == 0.0] - 0.0\n",
    "            u_at_1 = model_outputs_bc[x_vals == 1.0] - 0.0\n",
    "            if u_at_0.numel() > 0: loss += torch.mean(u_at_0**2)\n",
    "            if u_at_1.numel() > 0: loss += torch.mean(u_at_1**2)\n",
    "            return loss\n",
    "        def get_ground_truth(self,inputs,kappa_value):\n",
    "            x_np = inputs['x'].detach().cpu().numpy()\n",
    "            u_true_np = self.analytical_sol_np(x_np)\n",
    "            return torch.tensor(u_true_np, dtype=torch.float32, device=inputs['x'].device).reshape_as(inputs['x'])\n",
    "        def get_required_derivative_orders(self) -> Dict[str, Dict[Tuple[str, ...], int]]:\n",
    "            return {\n",
    "                'u': { # Assuming output_vars = ['u']\n",
    "                    ('x',): 2\n",
    "                }\n",
    "    }\n",
    "\n",
    "    # --- Setup for ExperimentRunner ---\n",
    "    pde_instances_map = {\n",
    "        \"TrivialLinear\": TrivialLinearPDE(),\n",
    "        \"Poisson\": PoissonPDE(),\n",
    "        # Add BurgersPDE(), KdVPDE() instances here once implemented\n",
    "    }\n",
    "\n",
    "    runner = ExperimentRunner(base_results_dir=\"experiment_data_final/\", pde_map=pde_instances_map, device=DEVICE)\n",
    "\n",
    "    # --- Define Experiment Sweeps ---\n",
    "    # For a quick test:\n",
    "    configs_to_run = []\n",
    "    test_pdes = [\"Poisson\"] # Or [\"TrivialLinear\", \"Poisson\"]\n",
    "    test_widths = [20, 50] # Reduced N for speed\n",
    "    test_kappas_poisson = [1.0] # Poisson kappa is fixed\n",
    "    test_activations = [\"tanh\"]\n",
    "    test_seeds = [1]\n",
    "    test_epochs = 2000 # Reduced for quick test\n",
    "\n",
    "    for pde_name in test_pdes:\n",
    "        kappas_for_this_pde = test_kappas_poisson # In a real scenario, fetch from a KAPPA_VALS_MAP\n",
    "        # if pde_name == \"Burgers\": kappas_for_this_pde = [10.0, 100.0] etc.\n",
    "\n",
    "        for kappa_v in kappas_for_this_pde:\n",
    "            for width_v in test_widths:\n",
    "                for act_v in test_activations:\n",
    "                    for seed_v in test_seeds:\n",
    "                        configs_to_run.append(ExperimentConfig(\n",
    "                            pde_name=pde_name,\n",
    "                            kappa_val=kappa_v,\n",
    "                            activation_str=act_v,\n",
    "                            seed=seed_v,\n",
    "                            depth=1, # SLN\n",
    "                            width=width_v,\n",
    "                            M_collocation_factor=10,\n",
    "                            collocation_scheme=\"uniform\", # or \"sobol\"\n",
    "                            optimizer_type=\"adam\",\n",
    "                            lr=1e-3,\n",
    "                            epochs=test_epochs,\n",
    "                        ))\n",
    "\n",
    "    for cfg in configs_to_run:\n",
    "        runner.run_single_experiment(cfg)\n",
    "\n",
    "    print(\"\\n--- Example: How to load and quickly check a result ---\")\n",
    "    if configs_to_run:\n",
    "        example_run_dir = runner._get_run_dir(configs_to_run[0])\n",
    "        try:\n",
    "            df_log = pd.read_csv(os.path.join(example_run_dir, \"training_log.csv\"))\n",
    "            print(f\"Log for first experiment ({configs_to_run[0].pde_name}, N={configs_to_run[0].width}):\")\n",
    "            print(df_log[['epoch', 'loss_total', 'L2_err_rel']].tail())\n",
    "            with open(os.path.join(example_run_dir, \"summary.json\"), 'r') as f:\n",
    "                summary = json.load(f)\n",
    "            print(\"\\nSummary of final metrics:\")\n",
    "            for k,v in summary['final_metrics'].items():\n",
    "                print(f\"  {k}: {v:.3e}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Could not find results for example run at: {example_run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd58e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
